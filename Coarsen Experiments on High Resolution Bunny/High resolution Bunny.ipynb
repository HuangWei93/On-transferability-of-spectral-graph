{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"High resolution Bunny.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"YRGBAuuyuoNf","colab_type":"code","outputId":"e429c76f-2365-4a99-a991-f8107eb44f73","executionInfo":{"status":"ok","timestamp":1576660921689,"user_tz":-60,"elapsed":675,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5vtIg3sdu1Nt","colab_type":"code","outputId":"5c5e805a-1623-4902-c3b8-bf06abb6b07c","executionInfo":{"status":"ok","timestamp":1576660921871,"user_tz":-60,"elapsed":844,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import sys\n","sys.path.append('/content/drive/My Drive/coarsen')\n","sys.path.append('/content/drive/My Drive/coarsen/data')\n","\n","sys.path.append('/content/drive/My Drive/Coarsen Experiments on High Resolution Bunny')\n","import utils as ut\n","\n","%load_ext autoreload\n","%autoreload 2\n","%aimport coarsening\n","\n","import inspect\n","import os\n","import joblib\n","import tensorflow as tf\n","import numpy as np\n","import h5py\n","import scipy.sparse.linalg as la\n","import scipy.sparse as sp\n","import scipy\n","import time\n","import pickle\n","import csv\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","%matplotlib inline\n","\n","import scipy.io as sio\n","import process_data\n","import sklearn.metrics\n","import sklearn.neighbors\n","import scipy.sparse\n","import scipy.sparse.linalg\n","import scipy.spatial.distance\n","import skimage.measure\n","from sklearn.preprocessing import normalize\n","np.seterr(divide='ignore', invalid='ignore')\n","\n","import matplotlib.cm\n","import random\n"],"execution_count":34,"outputs":[{"output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qCBPx2BFu1rw","colab_type":"code","colab":{}},"source":["#####################################\n","### UTILITY FUNCTIONS             ###\n","#####################################\n","def laplacian(W, normalized=True):\n","    \"\"\"\n","    Return the Laplacian of the weigth matrix.\n","    W: input adjacency or weight matrix.\n","    \n","    \"\"\"\n","\n","    # Degree matrix.\n","    d = W.sum(axis=0)\n","\n","    # Laplacian matrix.\n","    if not normalized:\n","        D = scipy.sparse.diags(d.A.squeeze(), 0)\n","        L = D - W\n","    else:\n","        d += np.spacing(np.array(0, W.dtype))\n","        d = 1 / np.sqrt(d)\n","        D = scipy.sparse.diags(d.A.squeeze(), 0)\n","        I = scipy.sparse.identity(d.size, dtype=W.dtype)\n","        L = I - D * W * D\n","\n","    # assert np.abs(L - L.T).mean() < 1e-9\n","    assert type(L) is scipy.sparse.csr.csr_matrix\n","    return L\n"," \n","\n","\n","def coarsen(A, levels, normalized):\n","    graphs, parents = coarsening.metis(A, levels) #Coarsen a graph multiple times using Graclus variation of the METIS algorithm. \n","                                                  #Basically, we randomly sort the nodes, we iterate on them and we decided to group each node\n","                                                  #with the neighbor having highest w_ij * 1/(\\sum_k w_ik) + w_ij * 1/(\\sum_k w_kj) \n","                                                  #i.e. highest sum of probabilities to randomly walk from i to j and from j to i.\n","                                                  #We thus favour strong connections (i.e. the ones with high weight wrt all the others for both nodes) \n","                                                  #in the choice of the neighbor of each node.\n","                    \n","                                                  #Construction is done a priori, so we have one graph for all the samples!\n","                    \n","                                                  #graphs = list of spare adjacency matrices (it contains in position \n","                                                  #          0 the original graph)\n","                                                  #parents = list of numpy arrays (every array in position i contains \n","                                                  #           the mapping from graph i to graph i+1, i.e. the idx of\n","                                                  #           node i in the coarsed graph -> that is, the idx of its cluster) \n","    perms = coarsening.compute_perm(parents) #Return a list of indices to reorder the adjacency and data matrices so\n","                                             #that two consecutive nodes correspond to neighbors that should be collapsed\n","                                             #to produce the coarsed version of the graph.\n","                                             #Fake nodes are appended for each node which is not grouped with anybody else\n","    laplacians = []\n","    for i,A in enumerate(graphs):\n","        M, M = A.shape\n","\n","        # We remove self-connections created by metis.\n","        A = A.tocoo()\n","        A.setdiag(0)\n","\n","        if i < levels: #if we have to pool the graph \n","            A = coarsening.perm_adjacency(A, perms[i]) #matrix A is here extended with the fakes nodes\n","                                                       #in order to do an efficient pooling operation\n","                                                       #in tensorflow as it was a 1D pooling\n","\n","        A = A.tocsr()\n","        A.eliminate_zeros()\n","        Mnew, Mnew = A.shape\n","        print('Layer {0}: M_{0} = |V| = {1} nodes ({2} added), |E| = {3} edges'.format(i, Mnew, Mnew-M, A.nnz//2))\n","\n","        L = laplacian(A, normalized)\n","        laplacians.append(L)\n","\n","    return laplacians, parents, perms[0] if len(perms) > 0 else None\n","\n","\n","\n","def poly_operator(L, r, coefficients):\n","    AA = sp.csr_matrix(L.shape)\n","    AA.setdiag(1.0)\n","    res = AA*coefficients[0]\n","    for k in range(r):\n","        AA = AA.dot(L)\n","        res = res + coefficients[k+1] * AA\n","    return res\n","\n","  \n","# Sampling operator\n","def construct_S_for_L(perm, m, n, times): # m is the dimension of the coarsened graph, n is the original dimension, times is time you coarsen graph.\n","    S = sp.lil_matrix((m,n))\n","    for i in range(m):\n","      count = 0\n","      for k in range(times):\n","        if perm[times*i + k] <n:\n","          count = count + 1\n","      for k in range(times):\n","        if perm[times*i + k] <n:\n","          S[i,perm[times*i+k]] = 1.0/np.sqrt(count)\n","    S = S.tocsr()\n","    return S\n","               \n","def convert_map(perm):\n","    new_perm = np.arange(len(perm))\n","    new_perm = new_perm[perm]\n","    new_perm = np.argsort(new_perm)\n","    return new_perm\n","\n","  \n","  \n","def convert_to_rgb(eigenvector):\n","    \"\"\" Input must be normalized eigenvector \"\"\"\n","    \n","    cmap = matplotlib.cm.get_cmap('coolwarm')\n","    rgba = cmap(eigenvector) \n","    rgba *= 255\n","    rgba = rgba.astype(int)   \n","    return rgba\n","  \n","def sigmoid(chosen_eigen, k):\n","   return 1/(1+np.exp(-1.0*k*chosen_eigen)) \n","  \n","def customized_normalize(chosen_eigen):\n","    \"\"\" Normalize eigenvector before shifting and than dividing by max entry\"\"\"\n","\n","    min_value = chosen_eigen.min(axis=0)\n","    chosen_eigen_shifted = chosen_eigen - min_value\n","\n","    max_value = chosen_eigen_shifted.max(axis=0)\n","    chosen_eigen_shifted_normalized = chosen_eigen_shifted / max_value\n","    return chosen_eigen_shifted_normalized\n","\n","\"\"\"\n","functions related to reading and writinging .off and .coff files\n","\"\"\"\n","def read_coff(file):\n","    if 'COFF' != file.readline().strip():\n","        raise('Not a valid COFF header')\n","    n_verts, n_faces, n_dontknow = tuple([int(s) for s in file.readline().strip().split(' ')])\n","    verts_colors = [[float(s) for s in file.readline().strip().split(' ')[:-1]] for i in range(n_verts)]\n","    faces = [[int(s) for s in file.readline().strip().split(' ')][1:] for i_face in range(n_faces)]\n","    return verts_colors, faces\n","\n","def read_off(file):\n","    if 'OFF' != file.readline().strip():\n","        raise('Not a valid OFF header')\n","    n_verts, n_faces, n_dontknow = tuple([int(s) for s in file.readline().strip().split(' ')])\n","    verts = [[float(s) for s in file.readline().strip().split(' ')] for i in range(n_verts)]\n","    faces = [[int(s) for s in file.readline().strip().split(' ')][1:] for i_face in range(n_faces)]\n","    return verts, faces\n","file = open(\"/content/drive/My Drive/Coarsen Experiments on High Resolution Bunny/high_resolution_bunny.off\", \"r\")\n","verts, faces = read_off(file)\n","coords = np.array(verts)\n","faces = np.array(faces)\n","file.close()\n","\n","def write_coff(x_shift, y_shift, z_shift, signal, name, scale):\n","  \"\"\"signal: original signal on bunny manifold\n","     name: name of your coff file to be saved\n","     scale: larger -> brighter\n","  \"\"\"\n","  #processing signal before visualization\n","  signal = signal.real\n","  signal_normalized = customized_normalize(signal)\n","  colors = convert_to_rgb(signal_normalized)\n","  outfile = open(\"/content/drive/My Drive/Coarsen Experiments on High Resolution Bunny/high_resolution_plots/\"+name+\".off\",\"w+\")\n","  \n","  #don't change this part\n","  outfile.write(\"COFF\\n\")\n","  outfile.write(\"{0} {1} 0\\n\".format(len(coords), len(faces)))\n","\n","  for i in range(len(coords)):\n","    outfile.write(str((coords[i,0] + x_shift)*scale) + \" \" + str((coords[i,1] + y_shift)*scale) + \" \" + str((coords[i,2] + z_shift)*scale) + \" \" + str(colors[i,0]) + \" \" + str(colors[i,1]) + \" \" + str(colors[i,2]) + \" \" + str(colors[i,3]) + \" \\n\")\n","  \n","  for i in range(len(faces)):\n","    outfile.write(\"3\" + \" \" + str(faces[i,0]) + \" \" + str(faces[i,1]) + \" \" + str(faces[i,2]) + \"\\n\")\n","  outfile.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o1i5ZbopiPmi","colab_type":"text"},"source":["##**PART ONE: FINE TUNE PARAMETERS**\n","---\n","\n","\n","1.   construct signal \n","2.   define filters \n","3.   change brightness"]},{"cell_type":"code","metadata":{"id":"YFHoYalaP1Y4","colab_type":"code","colab":{}},"source":["#########################################################################\n","### Construct Signal  manully  with Ron's method                      ###\n","#########################################################################\n","distance0 = coords[:,0] \n","distance0 = distance0 - np.min(distance0)\n","\n","\n","distance1 = coords[:,1] \n","distance1 = distance1 - np.min(distance1)\n","\n","\n","distance2 = coords[:,2] \n","distance2 = distance2 - np.min(distance2)\n","\n","diam = np.max((np.max((np.max(distance0), np.max(distance1))), np.max(distance2)))\n","distance0 = distance0/diam\n","distance1 = distance1/diam\n","distance2 = distance2/diam\n","\n","pi = 3.1415926\n","constructed_signal = distance0 * 0.0\n","coeffs = np.random.uniform(0,1,(10,10,10))\n","\n","for i in range(10):\n","    for j in range(10):\n","        for k in range(10):\n","            constructed_signal = constructed_signal + np.exp(-2j*pi*coeffs[i,j,k]) * np.exp(-2j*pi*(i+1)*distance0) * np.exp(-2j*pi*(j+1)*distance1) * np.exp(-2j*pi*(k+1)*distance2)\n","constructed_signal = constructed_signal #Complex number\n","constructed_signal = constructed_signal.real\n","constructed_signal = 1.0/(1.0+np.exp(-1.0*constructed_signal))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9CKQu_-QhOgj","colab_type":"code","colab":{}},"source":["\n","r = 4 # the order of polinomial\n","\"\"\"\n","Middle pass: 2*x*(2-x)^3\n","\"\"\"\n","poly_coefficients_middle = 2.0*np.array([0.0, 8.0, -12.0, 6.0, -1.0])\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FCG0w4SrYoP4","colab_type":"code","colab":{}},"source":["shift = 6.0\n","scale = 1.5"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TQnt2bVPfd9F","colab_type":"text"},"source":["##**PART TWO: WRITING COFF FILE**\n","*Do not change this part*\n","---\n","\n","\n","1.   original signal and interpolated signal.\n","2.   original signal after laplacian and interpolated coarsened signal after laplacian.\n","3.   original signal after filters and interpolated coarsened signal after filters.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"70CVEuP4u1ut","colab_type":"code","colab":{}},"source":["#####################################\n","### IMPORT BUNNY ADJACENCY MATRIX ###\n","#####################################\n","A = sp.load_npz('/content/drive/My Drive/Coarsen Experiments on High Resolution Bunny/High_Resolution_Bunny_Adj.npz')\n","L = laplacian(A, True) # Normalized Laplacian"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D2jB-Sxw_WcJ","colab_type":"code","outputId":"cccb5846-0877-4eea-a26a-7ca8cec1ab61","executionInfo":{"status":"ok","timestamp":1576660934549,"user_tz":-60,"elapsed":13490,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["########################\n","### GRAPH COARSENING ###\n","########################\n","np.random.seed(0)\n","\n","dim_graph = A.shape[0]\n","\n","nonormal_laplacians, parents, perm = coarsen(A, 3, True)\n","\n","new_perm = convert_map(perm)\n","\n","S_L1 = construct_S_for_L(perm, nonormal_laplacians[1].shape[0], dim_graph, 2)\n","S_L2 = construct_S_for_L(perm, nonormal_laplacians[2].shape[0], dim_graph, 4)\n","S_L3 = construct_S_for_L(perm, nonormal_laplacians[3].shape[0], dim_graph, 8)\n","\n","L_coarse1 = (S_L1.dot(L)).dot(S_L1.transpose())\n","L_coarse2 = (S_L2.dot(L)).dot(S_L2.transpose())\n","L_coarse3 = (S_L3.dot(L)).dot(S_L3.transpose())"],"execution_count":40,"outputs":[{"output_type":"stream","text":["Layer 0: M_0 = |V| = 42120 nodes (7303 added), |E| = 104445 edges\n","Layer 1: M_1 = |V| = 21060 nodes (2418 added), |E| = 55920 edges\n","Layer 2: M_2 = |V| = 10530 nodes (648 added), |E| = 29640 edges\n","Layer 3: M_3 = |V| = 5265 nodes (0 added), |E| = 15788 edges\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ea2__F9XFPvj","colab_type":"code","colab":{}},"source":["#####################################\n","### COARSENING CONSTRUCTED SIGNAL ###\n","#####################################\n","\n","signal_coarsen1 = S_L1.dot(constructed_signal)\n","signal_coarsen2 = S_L2.dot(constructed_signal)\n","signal_coarsen3 = S_L3.dot(constructed_signal)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J36MdV5BDZ2T","colab_type":"code","colab":{}},"source":["##############################################################\n","### CREATE COLORED ORIGINAL AND INTERPOLATED BUNNY.OFF     ###\n","##############################################################\n","interpolated_signal_coarsen1 = (S_L1.transpose()).dot(signal_coarsen1)\n","interpolated_signal_coarsen2 = (S_L2.transpose()).dot(signal_coarsen2)\n","interpolated_signal_coarsen3 = (S_L3.transpose()).dot(signal_coarsen3)\n","write_coff(-shift, shift, 0, constructed_signal, 'constructed_signal', scale)\n","write_coff(-shift, 0, 0, interpolated_signal_coarsen1, 'interpolated_signal_coarsen1', scale)\n","write_coff(-shift, -shift, 0, interpolated_signal_coarsen2, 'interpolated_signal_coarsen2', scale)\n","write_coff(-shift, -2*shift, 0, interpolated_signal_coarsen3, 'interpolated_signal_coarsen3', scale)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zEb_kFqCVCBL","colab_type":"code","colab":{}},"source":["######################################################\n","### APPLYING LAPLACIAN ON FINE AND COARSE MANIFOLD ###\n","######################################################\n","signal_after_laplacian = L.dot(constructed_signal)\n","signal_coarsen_after_laplacian1 = L_coarse1.dot(signal_coarsen1)\n","signal_coarsen_after_laplacian2 = L_coarse2.dot(signal_coarsen2)\n","signal_coarsen_after_laplacian3 = L_coarse3.dot(signal_coarsen3)\n","interpolated_signal_coarsen_after_laplacian1 = (S_L1.transpose()).dot(signal_coarsen_after_laplacian1)\n","interpolated_signal_coarsen_after_laplacian2 = (S_L2.transpose()).dot(signal_coarsen_after_laplacian2)\n","interpolated_signal_coarsen_after_laplacian3 = (S_L3.transpose()).dot(signal_coarsen_after_laplacian3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8WuKoCnkWYPI","colab_type":"code","colab":{}},"source":["######################################################\n","### CREATE COLORED AFTER LAPLACIAN BUNNY .OFF      ###\n","######################################################\n","write_coff(shift,shift,0,signal_after_laplacian, 'signal_after_laplacian', scale)\n","write_coff(shift,0,0,interpolated_signal_coarsen_after_laplacian1, 'interpolated_signal_coarsen_after_laplacian1', scale)\n","write_coff(shift,-shift,0,interpolated_signal_coarsen_after_laplacian2, 'interpolated_signal_coarsen_after_laplacian2', scale)\n","write_coff(shift,-2*shift,0,interpolated_signal_coarsen_after_laplacian3, 'interpolated_signal_coarsen_after_laplacian3', scale)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0opUGOtWE5CM","colab_type":"code","outputId":"591288bb-3b45-4250-e2b4-79197f01da9f","executionInfo":{"status":"ok","timestamp":1576660941189,"user_tz":-60,"elapsed":20106,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["######################################\n","### DEFINE MIDDLE PASS FILTERS     ###\n","######################################\n","poly_operator_middle = poly_operator(L, r, poly_coefficients_middle)\n","poly_operator_coarse_middle1 = poly_operator(L_coarse1, r, poly_coefficients_middle)\n","poly_operator_coarse_middle2 = poly_operator(L_coarse2, r, poly_coefficients_middle)\n","poly_operator_coarse_middle3 = poly_operator(L_coarse3, r, poly_coefficients_middle)"],"execution_count":45,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n","  self._set_arrayXarray(i, j, x)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"pXKKfMCrF7yv","colab_type":"code","colab":{}},"source":["###################################################\n","### APPLYING FILTER ON FINE AND COARSE MANIFOLD ###\n","###################################################\n","\n","signal_middle_filter = poly_operator_middle.dot(constructed_signal)\n","signal_coarsen_then_middle_filter1 = poly_operator_coarse_middle1.dot(signal_coarsen1)\n","signal_coarsen_then_middle_filter2 = poly_operator_coarse_middle2.dot(signal_coarsen2)\n","signal_coarsen_then_middle_filter3 = poly_operator_coarse_middle3.dot(signal_coarsen3)\n","interpolated_signal_coarsen_then_middle_filter1 = (S_L1.transpose()).dot(signal_coarsen_then_middle_filter1)\n","interpolated_signal_coarsen_then_middle_filter2 = (S_L2.transpose()).dot(signal_coarsen_then_middle_filter2)\n","interpolated_signal_coarsen_then_middle_filter3 = (S_L3.transpose()).dot(signal_coarsen_then_middle_filter3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"84aQcaEzisJW","colab_type":"code","colab":{}},"source":["#coarsen_coords = S_for_features.dot(coords)\n","#from numpy import savetxt\n","#savetxt('/content/drive/My Drive/Coarsen Experiments on Bunny/coarsen_bunny_coordinates.csv', coarsen_coords, delimiter=' ')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AwB8a8MokekM","colab_type":"code","colab":{}},"source":["######################################################\n","### CREATE COLORED AFTER FILTERS BUNNY .OFF        ###\n","######################################################\n","write_coff(0,shift,0,signal_middle_filter, 'signal_middle_filter', scale)\n","write_coff(0,0,0,interpolated_signal_coarsen_then_middle_filter1, 'interpolated_signal_coarsen_then_middle_filter1', scale)\n","write_coff(0,-shift,0,interpolated_signal_coarsen_then_middle_filter2, 'interpolated_signal_coarsen_then_middle_filter2', scale)\n","write_coff(0,-2*shift,0,interpolated_signal_coarsen_then_middle_filter3, 'interpolated_signal_coarsen_then_middle_filter3', scale)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vjW5d7FYPAl6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}