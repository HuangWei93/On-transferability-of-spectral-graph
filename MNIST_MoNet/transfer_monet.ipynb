{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transfer_monet.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"KS9IyBod19SN","colab_type":"code","outputId":"f7820605-b2ad-4f85-d35c-9090e000ba70","executionInfo":{"status":"ok","timestamp":1581500029357,"user_tz":-60,"elapsed":1862,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"exIMORWc2B3i","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append('/content/gdrive/My Drive/MNIST_MoNet')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LxzX4ti213Rh","colab_type":"code","outputId":"9de17ff0-3813-4235-b3e6-a6f409323bab","executionInfo":{"status":"ok","timestamp":1581500032493,"user_tz":-60,"elapsed":4978,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":63}},"source":["#import graph, coarsening, utils\n","%load_ext autoreload\n","%autoreload 1\n","%aimport graph\n","%aimport coarsening\n","%aimport utils\n","\n","import tensorflow as tf\n","import time, shutil\n","import numpy as np\n","import os, collections, sklearn\n","import scipy.sparse as sp\n","import scipy\n","import matplotlib.pyplot as plt\n","import networkx as nx\n","import cv2\n","%matplotlib inline"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"LytKMiTt13Ro","colab_type":"code","colab":{}},"source":["#Definition of some flags useful later in the code\n","\n","flags = tf.app.flags\n","FLAGS = flags.FLAGS\n","# Graphs.\n","flags.DEFINE_string('f', '', 'kernel')\n","flags.DEFINE_integer('number_edges', 8, 'Graph: minimum number of edges per vertex.')\n","flags.DEFINE_string('metric', 'euclidean', 'Graph: similarity measure (between features).')\n","flags.DEFINE_bool('normalized_laplacian', True, 'Graph Laplacian: normalized.')\n","# Directories.\n","flags.DEFINE_string('dir_data', 'data_mnist', 'Directory to store data.')\n","\n","new_dim = 24\n","pool_step = 2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"BBwQSTNh13Rs","colab_type":"code","outputId":"fbb700f7-6f73-47f2-937f-e0341268cac2","executionInfo":{"status":"ok","timestamp":1581500032495,"user_tz":-60,"elapsed":4962,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":241}},"source":["#Here we proceed at computing the original grid where the images live and the various coarsening that are applied\n","#for each level\n","def get_rho(dist, idx):\n","    \"\"\"Return the adjacency matrix of a kNN graph.\"\"\"\n","    M, k = dist.shape\n","    assert M, k == idx.shape\n","    assert dist.min() >= 0\n","\n","    # Weight matrix.\n","    I = np.arange(0, M).repeat(k)\n","    J = idx.reshape(M*k)\n","    V = dist.reshape(M*k)\n","    rhos = scipy.sparse.coo_matrix((V, (I, J)), shape=(M, M))\n","\n","    # No self-connections.\n","    rhos.setdiag(0)\n","\n","    # Non-directed graph.\n","    bigger = rhos.T > rhos\n","    rhos = rhos - rhos.multiply(bigger) + rhos.T.multiply(bigger)\n","\n","    assert rhos.nnz % 2 == 0\n","    assert np.abs(rhos - rhos.T).mean() < 1e-10\n","    assert type(rhos) is scipy.sparse.csr.csr_matrix\n","    return rhos\n","def get_theta(angle, idx):\n","    \"\"\"Return the adjacency matrix of a kNN graph.\"\"\"\n","    M, k = angle.shape\n","    assert M, k == idx.shape\n","\n","    # Weight matrix.\n","    I = np.arange(0, M).repeat(k)\n","    J = idx.reshape(M*k)\n","    V = angle.reshape(M*k)\n","    theta = scipy.sparse.coo_matrix((V, (I, J)), shape=(M, M))\n","\n","    # No self-connections.\n","    theta.setdiag(0)\n","\n","    # Non-directed graph.\n","    bigger = theta.T > theta\n","    theta = theta - theta.multiply(bigger) + theta.T.multiply(bigger)\n","\n","    assert theta.nnz % 2 == 0\n","    assert np.abs(theta - theta.T).mean() < 1e-10\n","    assert type(theta) is scipy.sparse.csr.csr_matrix\n","    return theta\n","def grid_graph(m):\n","    z = graph.grid(m)\n","    dist, angle, idx = graph.distance_sklearn_metrics(z, k=FLAGS.number_edges, metric=FLAGS.metric) \n","    #dist contains the distance of the 8 nearest neighbors for each node sorted in ascending order\n","    #idx contains the indexes of the 8 nearest for each node sorted in ascending order by distance\n","\n","    rhos = get_rho(dist, idx)\n","    rhos.setdiag(0.0)\n","    rhos = rhos.tocoo()\n","\n","    theta = get_theta(angle, idx)\n","    theta.setdiag(0.0)\n","    theta = theta.tocoo()\n","    return rhos, theta\n","\n","def get_laplacian(m):\n","    z = graph.grid(m)\n","    dist, angle, idx = graph.distance_sklearn_metrics(z, k=FLAGS.number_edges, metric=FLAGS.metric) \n","    #dist contains the distance of the 8 nearest neighbors for each node sorted in ascending order\n","    #idx contains the indexes of the 8 nearest for each node sorted in ascending order by distance\n","\n","    A = graph.adjacency(dist, idx)\n","    L = graph.laplacian(A, normalized=FLAGS.normalized_laplacian)\n","    L = L.tocoo()\n","    return L\n","np.random.seed(0)\n","rho1, theta1 = grid_graph(new_dim)\n","rho2, theta2 = grid_graph(new_dim//pool_step)\n","rho3, theta3 = grid_graph(new_dim//pool_step**2)\n","Rhos = []\n","Rhos.append(rho1)\n","Rhos.append(rho2)\n","Rhos.append(rho3)\n","Thetas = []\n","Thetas.append(theta1)\n","Thetas.append(theta2)\n","Thetas.append(theta3)\n","\n","L = []\n","L.append(get_laplacian(new_dim))\n","L.append(get_laplacian(new_dim//pool_step))\n","L.append(get_laplacian(new_dim//pool_step**2))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n","  self._set_arrayXarray(i, j, x)\n","/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n","  self._set_arrayXarray(i, j, x)\n","/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n","  self._set_arrayXarray(i, j, x)\n","/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n","  self._set_arrayXarray(i, j, x)\n","/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n","  self._set_arrayXarray(i, j, x)\n","/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n","  self._set_arrayXarray(i, j, x)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"DPx0oazL13R5","colab_type":"code","outputId":"cc710852-d715-44de-f179-89386fbda274","executionInfo":{"status":"ok","timestamp":1581500032869,"user_tz":-60,"elapsed":5324,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":377}},"source":["#loading of MNIST dataset\n","\n","from tensorflow.examples.tutorials.mnist import input_data\n","mnist = input_data.read_data_sets(FLAGS.dir_data, one_hot=False)\n","\n","train_data = mnist.train.images.astype(np.float32)\n","val_data = mnist.validation.images.astype(np.float32) #the first 5K samples of the training dataset \n","                                                      #are used for validation\n","test_data = mnist.test.images.astype(np.float32)\n","train_labels = mnist.train.labels\n","val_labels = mnist.validation.labels\n","test_labels = mnist.test.labels\n","new_train_data = np.zeros((train_data.shape[0], new_dim*new_dim))\n","new_val_data = np.zeros((val_data.shape[0], new_dim*new_dim))\n","new_test_data = np.zeros((test_data.shape[0], new_dim*new_dim))\n","for i in range(train_data.shape[0]):\n","    im = train_data[i,:].reshape(28,28)\n","    im = cv2.resize(im, (new_dim,new_dim), interpolation = cv2.INTER_LINEAR)\n","    new_train_data[i,:] = im.reshape(1,-1)\n","for i in range(val_data.shape[0]):\n","    im = val_data[i,:].reshape(28,28)\n","    im = cv2.resize(im, (new_dim,new_dim), interpolation = cv2.INTER_LINEAR)\n","    new_val_data[i,:] = im.reshape(1,-1)\n","for i in range(test_data.shape[0]):\n","    im = test_data[i,:].reshape(28,28)\n","    im = cv2.resize(im, (new_dim,new_dim), interpolation = cv2.INTER_LINEAR)\n","    new_test_data[i,:] = im.reshape(1,-1)\n","\n","t_start = time.time()\n","train_data = new_train_data\n","val_data = new_val_data\n","test_data = new_test_data\n","print('Execution time: {:.2f}s'.format(time.time() - t_start))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-6-febc93d49cc6>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please write your own downloading logic.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting data_mnist/train-images-idx3-ubyte.gz\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting data_mnist/train-labels-idx1-ubyte.gz\n","Extracting data_mnist/t10k-images-idx3-ubyte.gz\n","Extracting data_mnist/t10k-labels-idx1-ubyte.gz\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","Execution time: 0.00s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L2u3-wzX13R7","colab_type":"code","colab":{}},"source":["class MoNet:\n","    \"\"\"\n","    The neural network model.\n","    \"\"\"\n","    \n","    #Helper functions used for constructing the model\n","    def _weight_variable(self, shape, regularization=True): \n","        \"\"\"Initializer for the weights\"\"\"\n","        \n","        initial = tf.truncated_normal_initializer(0, 0.1)\n","        var = tf.get_variable('weights', shape, tf.float32, initializer=initial)\n","        if regularization: #append the loss of the current variable to the regularization term \n","            self.regularizers.append(tf.nn.l2_loss(var))\n","        return var\n","    \n","    def _bias_variable(self, shape, regularization=True):\n","        \"\"\"Initializer for the bias\"\"\"\n","        \n","        initial = tf.constant_initializer(0.1)\n","        var = tf.get_variable('bias', shape, tf.float32, initializer=initial)\n","        if regularization:\n","            self.regularizers.append(tf.nn.l2_loss(var))\n","        return var\n","    def _mu_variable(self, shape, regularization=True): \n","        \"\"\"Initializer for the weights\"\"\"\n","        \n","        initial = tf.random_uniform_initializer(0,0.1)\n","        var = tf.get_variable('gaussian_mu', shape, tf.float32, initializer=initial)\n","        if regularization: #append the loss of the current variable to the regularization term \n","            self.regularizers.append(tf.nn.l2_loss(var))\n","        return var  \n","    def _sigma_variable(self, shape, regularization=True): \n","        \"\"\"Initializer for the weights\"\"\"\n","        \n","        initial = tf.random_uniform_initializer(0,1)\n","        var = tf.get_variable('gaussian_sigma', shape, tf.float32, initializer=initial)\n","        if regularization: #append the loss of the current variable to the regularization term \n","            self.regularizers.append(tf.nn.l2_loss(var))\n","        return var \n","\n","    def frobenius_norm(self, tensor): \n","        \"\"\"Computes the frobenius norm for a given tensor\"\"\"\n","        \n","        square_tensor = tf.square(tensor)\n","        tensor_sum = tf.reduce_sum(square_tensor)\n","        frobenius_norm = tf.sqrt(tensor_sum)\n","        return frobenius_norm\n","    \n","    \n","    def count_no_weights(self):\n","        total_parameters = 0\n","        for variable in tf.trainable_variables():\n","            # shape is an array of tf.Dimension\n","            shape = variable.get_shape()\n","            variable_parameters = 1\n","            for dim in shape:\n","                variable_parameters *= dim.value\n","            total_parameters += variable_parameters\n","        print('#weights in the model: %d' % (total_parameters,))\n","\n","    \n","    #Modules used by the graph convolutional network\n","    def MoConv(self, x, L, Rho, Theta, Fout, n_gaussian): \n","        \"\"\"Applies Monet over the graph (i.e. it makes a spatial convolution)\"\"\"\n","        #Fout is the number of output features\n","        #Rho is pseudo-coordinates\n","        #n_gaussian is dimensionality of the extracted patch\n","        N, M, Fin = x.get_shape()  # N is the number of images\n","                                   # M the number of vertices in the images\n","                                   # Fin the number of features\n","        N, M, Fin = int(N), int(M), int(Fin)\n","        \n","        gaussian_filter = self.gaussianlayer(L, Rho, Theta, n_gaussian) #(M*n_gaussian, M)\n","        W = self._weight_variable([n_gaussian*Fin, Fout], regularization=False) #(n_gaussian*Fin, Fout)\n","        x = tf.transpose(x,[0,2,1]) \n","        x = tf.reshape(x,[N*Fin, M])\n","        x = tf.transpose(x,[1,0]) #(M, N*Fin)\n","        gaussian_feats = tf.matmul(gaussian_filter, x) #(M*n_gaussian, N*Fin)\n","        gaussian_feats = tf.transpose(gaussian_feats, [1,0])\n","        gaussian_feats = tf.reshape(gaussian_feats, [N, Fin,M*n_gaussian])\n","        gaussian_feats = tf.transpose(gaussian_feats, [0,2,1]) \n","        gaussian_feats = tf.reshape(gaussian_feats, [N, M, Fin*n_gaussian])\n","        gaussian_feats = tf.reshape(gaussian_feats, [N*M, Fin*n_gaussian])\n","        output = tf.matmul(gaussian_feats,W) #(N*M, Fout)\n","        output = tf.reshape(output, [N, M, Fout])\n","        return output\n","    \n","    def gaussianlayer(self, L, Rho, Theta, n_gaussian):\n","        #Rho is pseudo-coordinates\n","        #n_gaussian is dimensionality of the extracted patch\n","        M, _= Rho.shape\n","        M = int(M)\n","        mask = tf.sparse.SparseTensor(L.indices, [1.0]*(L.values.shape[0]), [M,M])\n","        mask = tf.sparse.to_dense(mask)\n","        mask = tf.expand_dims(mask,2)\n","        with tf.variable_scope('Rho'):\n","            Rho = tf.sparse.to_dense(Rho)\n","            Rho = tf.expand_dims(Rho, 2)\n","            #initialize mu and sigma\n","            rho_mu = tf.constant(0.0, shape=[1, 1, n_gaussian])\n","            rho_sigma = self._sigma_variable([1, 1, n_gaussian], regularization=True)\n","            #compute coefficience of MoNet patch operator\n","            g_weight_rho = tf.exp((Rho - rho_mu)**2*(-0.5/rho_sigma**2))\n","            #mask\n","            g_weight_rho = g_weight_rho * mask \n","        with tf.variable_scope('Theta'):\n","            Theta = tf.sparse.to_dense(Theta)\n","            Theta = tf.expand_dims(Theta, 2)\n","            #initialize mu and sigma\n","            theta_mu = tf.constant(0.0, shape=[1, 1, n_gaussian])\n","            theta_sigma = self._sigma_variable([1, 1, n_gaussian], regularization=True)\n","            #compute coefficience of MoNet patch operator\n","            g_weight_theta = tf.exp((Theta - theta_mu)**2*(-0.5/theta_sigma**2))\n","            #mask\n","            g_weight_theta = g_weight_theta * mask \n","        #normolize\n","        g_weight = g_weight_theta * g_weight_rho\n","        g_weight = g_weight/tf.reduce_sum(g_weight,1,keepdims=True)\n","        #reshape\n","        g_weight = tf.transpose(g_weight,(1,0,2))\n","        g_weight = tf.reshape(g_weight,(M, M*n_gaussian))\n","        return tf.transpose(g_weight,(1,0))\n","    \n","    def b1relu(self, x):\n","        \"\"\"Applies bias and ReLU. One bias per filter.\"\"\"\n","        N, M, F = x.get_shape()\n","        b = self._bias_variable([1, 1, int(F)], regularization=False)\n","        return tf.nn.relu(x + b) #add the bias to the convolutive layer\n","\n","\n","    def mpool1(self, x, p):\n","        \"\"\"Max pooling of size p. Should be a power of 2 (this is possible thanks to the reordering we previously did).\"\"\"\n","        if p > 1:\n","            x = tf.expand_dims(x, 3)  # shape = N x M x F x 1\n","            x = tf.nn.max_pool(x, ksize=[1,p,1,1], strides=[1,p,1,1], padding='SAME')\n","            return tf.squeeze(x, [3])  # shape = N x M/p x F\n","        else:\n","            return x\n","\n","    def fc(self, x, Mout, relu=True):\n","        \"\"\"Fully connected layer with Mout features.\"\"\"\n","        N, Min = x.get_shape()\n","        W = self._weight_variable([int(Min), Mout], regularization=True)\n","        b = self._bias_variable([Mout], regularization=True)\n","        x = tf.matmul(x, W) + b\n","        return tf.nn.relu(x) if relu else x\n","    \n","    #function used for extracting the result of our model\n","    def _inference(self, x, dropout): #definition of the model\n","        \n","        # Graph convolutional layers.\n","        x = tf.expand_dims(x, 2)  # N x M x F=1\n","        rec_size = [new_dim, new_dim//pool_step, new_dim//pool_step**2]\n","        for i in range(len(self.p)):\n","            with tf.variable_scope('cgconv{}'.format(i+1)):\n","                with tf.name_scope('filter'):\n","                    x = self.MoConv(x, self.L[i], self.Rhos[i], self.Thetas[i], self.F[i], self.K[i])\n","                with tf.name_scope('bias_relu'):\n","                    x = self.b1relu(x)\n","                with tf.name_scope('pooling'):\n","                    N, M, F = x.get_shape()\n","                    x = tf.reshape(x, [N, rec_size[i],rec_size[i], F])\n","                    x = tf.contrib.layers.instance_norm(x)\n","                    x = tf.nn.max_pool(x, ksize=[1,self.p[i],self.p[i],1], strides=[1,self.p[i],self.p[i],1], padding='SAME')\n","                    x = tf.reshape(x, [N, -1, F])\n","         \n","        # Fully connected hidden layers.\n","        N, M, F = x.get_shape()\n","        x = tf.reshape(x, [int(N), int(M*F)])  # N x M\n","        if self.M:\n","            for i,M in enumerate(self.M[:-1]): #apply a fully connected layer for each layer defined in M\n","                                              #(we discard the last value in M since it contains the number of classes we have\n","                                              #to predict)\n","                with tf.variable_scope('fc{}'.format(i+1)):\n","                    x = self.fc(x, M)\n","                    x = tf.nn.dropout(x, dropout)\n","            \n","            # Logits linear layer, i.e. softmax without normalization.\n","            with tf.variable_scope('logits'):\n","                x = self.fc(x, self.M[-1], relu=False)\n","        return x\n","    \n","    def convert_coo_to_sparse_tensor(self, L):\n","        indices = np.column_stack((L.row, L.col))\n","        L = tf.SparseTensor(indices, L.data.astype('float32'), L.shape)\n","        L = tf.sparse_reorder(L)\n","        return L\n","    \n","    def __init__(self, p, K, F, M, M_0, batch_size, L, Rhos, Thetas,\n","                 decay_steps, decay_rate, learning_rate=1e-4, momentum=0.9, regularization=5e-4, clip_norm=1e1,\n","                 idx_gpu = '/gpu:0'):\n","        self.regularizers = list() #list of regularization l2 loss for multiple variables\n","        \n","        self.p = p #dimensions of the pooling layers\n","        self.K = K #List of polynomial orders, i.e. filter sizes or number of hops\n","        self.F = F #Number of features of convolutional layers\n","        \n","        self.M = M #Number of neurons in fully connected layers\n","        \n","        self.M_0 = M_0 #number of elements in the first graph \n","        \n","        self.batch_size = batch_size\n","        \n","        #definition of some learning parameters\n","        self.decay_steps = decay_steps\n","        self.decay_rate = decay_rate\n","        self.learning_rate = learning_rate\n","        self.regularization = regularization\n","        \n","        with tf.Graph().as_default() as g:\n","                self.graph = g\n","                tf.set_random_seed(0)\n","                with tf.device(idx_gpu):\n","                        #definition of placeholders\n","                        self.Rhos = [self.convert_coo_to_sparse_tensor(Rho) for Rho in Rhos]\n","                        self.Thetas = [self.convert_coo_to_sparse_tensor(Theta) for Theta in Thetas]\n","                        self.L = [self.convert_coo_to_sparse_tensor(c_L) for c_L in L]\n","                        self.ph_data = tf.placeholder(tf.float32, (self.batch_size, M_0), 'data')\n","                        self.ph_labels = tf.placeholder(tf.int32, (self.batch_size), 'labels')\n","                        self.ph_dropout = tf.placeholder(tf.float32, (), 'dropout')\n","                    \n","                        #Model construction\n","                        self.logits = self._inference(self.ph_data, self.ph_dropout)\n","                        \n","                        #Definition of the loss function\n","                        with tf.name_scope('loss'):\n","                            self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, \n","                                                                                                labels=self.ph_labels)\n","                            self.cross_entropy = tf.reduce_mean(self.cross_entropy)\n","                        with tf.name_scope('regularization'):\n","                            if self.M:\n","                                self.regularization *= tf.add_n(self.regularizers)\n","                        self.loss = self.cross_entropy + self.regularization\n","                        \n","                        # Create a session for running Ops on the Graph.\n","                        config = tf.ConfigProto(allow_soft_placement = True)\n","                        config.gpu_options.allow_growth = True\n","                        self.session = tf.Session(config=config)\n","                        #Solver Definition\n","                        with tf.name_scope('training'):\n","                            # Learning rate.\n","                            self.global_step = tf.Variable(0, name='global_step', trainable=False) #used for counting how many iterations we have done\n","                            if decay_rate != 1: #applies an exponential decay of the lr wrt the number of iterations done\n","                                learning_rate = tf.train.exponential_decay(\n","                                        learning_rate, self.global_step, decay_steps, decay_rate, staircase=True)\n","                            # Optimizer.\n","                            if momentum == 0:\n","                                optimizer = tf.train.AdamOptimizer(learning_rate)\n","                            else: #applies momentum for increasing the robustness of the gradient \n","                                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n","                            #grads = optimizer.compute_gradients(self.loss)\n","                            tvars = tf.trainable_variables()\n","                            #grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), clip_norm)\n","                            variable_names = [v.name for v in tvars]\n","                            variables_to_restore = {v.name.split(\":\")[0]: v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)}\n","                            skip_pretrained_var = []\n","                            variables_to_restore = {v: variables_to_restore[v] for v in variables_to_restore if not any(x in v for x in skip_pretrained_var)}\n","                            if variables_to_restore:\n","                                saver_pre_trained = tf.train.Saver(var_list=variables_to_restore)\n","                                ckpt = tf.train.get_checkpoint_state('./gdrive/My Drive/MNIST_MoNet/model_save/')\n","                                if ckpt != None:\n","                                  saver_pre_trained.restore(self.session, ckpt.model_checkpoint_path) \n","                                  print('Relodaing parameters')\n","                            train_vars = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) if any(x in v.name for x in skip_pretrained_var)]\n","\n","                            grads, variables = zip(*optimizer.compute_gradients(self.loss, var_list=tvars))\n","                            grads, _ = tf.clip_by_global_norm(grads, clip_norm)\n","                            self.op_gradients = optimizer.apply_gradients(zip(grads, variables), \n","                                                                          global_step=self.global_step)\n","                            \n","                        #Computation of the norm gradients (useful for debugging)\n","                        self.var_grad = tf.gradients(self.loss, tf.trainable_variables())\n","                        self.norm_grad = self.frobenius_norm(tf.concat([tf.reshape(g, [-1]) for g in self.var_grad], 0))\n","\n","                        #Extraction of the predictions and computation of accuracy\n","                        self.predictions = tf.cast(tf.argmax(self.logits, dimension=1), tf.int32)\n","                        self.accuracy = 100 * tf.contrib.metrics.accuracy(self.predictions, self.ph_labels)\n","        \n","                        uninitialized_vars = []\n","                        for var in tf.all_variables():\n","                            try:\n","                                self.session.run(var)\n","                            except tf.errors.FailedPreconditionError:\n","                                uninitialized_vars.append(var)\n","\n","                        init_new_vars_op = tf.initialize_variables(uninitialized_vars)\n","                        self.session.run(init_new_vars_op) \n","                        self.saver = tf.train.Saver(tf.global_variables())\n","                        self.count_no_weights()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wl3L11Wv13R8","colab_type":"code","colab":{}},"source":["#Convolutional parameters\n","p = [pool_step, pool_step, new_dim//pool_step**2]   #Dimensions of the pooling layers\n","K = [18, 18, 18] #List of gaussians distribution\n","F = [32, 32, 64] #Number of features of convolutional layers\n","\n","#FC parameters\n","C = max(mnist.train.labels) + 1 #Number of classes we have\n","M = [512, C] #Number of neurons in fully connected layers\n","\n","#Solver parameters\n","batch_size = 1000\n","decay_steps = mnist.train.num_examples / batch_size #number of steps to do before decreasing the learning rate\n","decay_rate = 0.95 #how much decreasing the learning rate\n","learning_rate = 0.02\n","momentum = 0.9\n","regularization = 5e-4\n","\n","#Definition of keep probabilities for dropout layers\n","dropout_training = 0.5\n","dropout_val_test = 1.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ULADZIn13R-","colab_type":"code","outputId":"dcdc19a7-3eb4-4724-b02a-232302411d85","executionInfo":{"status":"ok","timestamp":1581500037137,"user_tz":-60,"elapsed":9570,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":479}},"source":["#Construction of the learning obj\n","M_0 = Rhos[0].shape[0] #number of elements in the first graph\n","learning_obj = MoNet(p, K, F, M, M_0, batch_size, L, Rhos, Thetas,\n","                       decay_steps, decay_rate,\n","                       learning_rate=learning_rate, regularization=regularization,\n","                       momentum=momentum)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From <ipython-input-7-900f95cd80e9>:176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","INFO:tensorflow:Restoring parameters from ./gdrive/My Drive/MNIST_MoNet/model_save/model.ckpt-16501\n","Relodaing parameters\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From <ipython-input-7-900f95cd80e9>:276: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use the `axis` argument instead\n","WARNING:tensorflow:From <ipython-input-7-900f95cd80e9>:280: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n","Instructions for updating:\n","Please use tf.global_variables instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py:198: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n","Instructions for updating:\n","Use `tf.variables_initializer` instead.\n","#weights in the model: 94774\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"K93L1X2B13SA","colab_type":"code","outputId":"e54c1a8c-41c0-4ab3-de66-69b49e7a55c2","executionInfo":{"status":"ok","timestamp":1581500039953,"user_tz":-60,"elapsed":12376,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\n","        \n","#Validation Code\n","tic = time.time()\n","val_accuracy = 0\n","for begin in range(0, val_data.shape[0], batch_size):\n","    end = begin + batch_size\n","    end = min([end, val_data.shape[0]])\n","    \n","    #data extraction\n","    batch_data = np.zeros((end-begin, val_data.shape[1]))\n","    batch_data = val_data[begin:end,:]\n","    batch_labels = np.zeros(batch_size)\n","    batch_labels[:end-begin] = val_labels[begin:end]\n","    \n","    feed_dict = {learning_obj.ph_data: batch_data, \n","                  learning_obj.ph_labels: batch_labels,\n","                  learning_obj.ph_dropout: dropout_val_test}\n","    \n","    batch_accuracy = learning_obj.session.run(learning_obj.accuracy, feed_dict)\n","    val_accuracy += batch_accuracy*batch_data.shape[0]\n","val_accuracy = val_accuracy/val_data.shape[0]\n","val_time = time.time() - tic\n","msg = \"[VAL]acc = %4.2f (%3.2es)\" % (val_accuracy, val_time)\n","print(msg)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[VAL]acc = 8.32 (2.31e+00s)\n"],"name":"stdout"}]}]}