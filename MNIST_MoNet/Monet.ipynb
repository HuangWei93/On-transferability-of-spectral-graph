{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Monet.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"SDVm0oFuuXOk","colab_type":"code","outputId":"ae67cbb3-cb3e-4572-b6ba-99c3fd61d96b","executionInfo":{"status":"ok","timestamp":1581504532649,"user_tz":-60,"elapsed":546,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L9Ea1q7IukTq","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append('/content/gdrive/My Drive/MNIST_MoNet')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hT8lO-I5uVhx","colab_type":"code","outputId":"d17d4636-9dd3-442d-d7af-1ec0277c65ee","executionInfo":{"status":"ok","timestamp":1581504535198,"user_tz":-60,"elapsed":3074,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":63}},"source":["#import graph, coarsening, utils\n","%load_ext autoreload\n","%autoreload 1\n","%aimport graph\n","%aimport coarsening\n","%aimport utils\n","\n","import tensorflow as tf\n","import time, shutil\n","import numpy as np\n","import os, collections, sklearn\n","import scipy.sparse as sp\n","import scipy\n","import matplotlib.pyplot as plt\n","import networkx as nx\n","import cv2\n","%matplotlib inline"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"hQMwhDN_uVh3","colab_type":"code","colab":{}},"source":["#Definition of some flags useful later in the code\n","\n","flags = tf.app.flags\n","FLAGS = flags.FLAGS\n","# Graphs.\n","flags.DEFINE_string('f', '', 'kernel')\n","flags.DEFINE_integer('number_edges', 8, 'Graph: minimum number of edges per vertex.')\n","flags.DEFINE_string('metric', 'euclidean', 'Graph: similarity measure (between features).')\n","flags.DEFINE_bool('normalized_laplacian', True, 'Graph Laplacian: normalized.')\n","# Directories.\n","flags.DEFINE_string('dir_data', 'data_mnist', 'Directory to store data.')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FYTwenF9uVh6","colab_type":"code","outputId":"d974475f-d3bd-4d31-8b70-7cb9c019eb52","executionInfo":{"status":"ok","timestamp":1581504537714,"user_tz":-60,"elapsed":5571,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":241}},"source":["#Here we proceed at computing the original grid where the images live and the various coarsening that are applied\n","#for each level\n","\n","\n","def get_rho(dist, idx):\n","    \"\"\"Return the adjacency matrix of a \n","    0.graph.\"\"\"\n","    M, k = dist.shape\n","    assert M, k == idx.shape\n","    assert dist.min() >= 0\n","\n","    # Weight matrix.\n","    I = np.arange(0, M).repeat(k)\n","    J = idx.reshape(M*k)\n","    V = dist.reshape(M*k)\n","    rhos = scipy.sparse.coo_matrix((V, (I, J)), shape=(M, M))\n","\n","    # No self-connections.\n","    rhos.setdiag(0)\n","\n","    # Non-directed graph.\n","    bigger = rhos.T > rhos\n","    rhos = rhos - rhos.multiply(bigger) + rhos.T.multiply(bigger)\n","\n","    assert rhos.nnz % 2 == 0\n","    assert np.abs(rhos - rhos.T).mean() < 1e-10\n","    assert type(rhos) is scipy.sparse.csr.csr_matrix\n","    return rhos\n","  \n","  \n","def get_theta(angle, idx):\n","    \"\"\"Return the adjacency matrix of a kNN graph.\"\"\"\n","    M, k = angle.shape\n","    assert M, k == idx.shape\n","\n","    # Weight matrix.\n","    I = np.arange(0, M).repeat(k)\n","    J = idx.reshape(M*k)\n","    V = angle.reshape(M*k)\n","    theta = scipy.sparse.coo_matrix((V, (I, J)), shape=(M, M))\n","\n","    # No self-connections.\n","    theta.setdiag(0)\n","\n","    # Non-directed graph.\n","    bigger = theta.T > theta\n","    theta = theta - theta.multiply(bigger) + theta.T.multiply(bigger)\n","\n","    assert theta.nnz % 2 == 0\n","    assert np.abs(theta - theta.T).mean() < 1e-10\n","    assert type(theta) is scipy.sparse.csr.csr_matrix\n","    return theta\n","  \n","  \n","def grid_graph(m):\n","    z = graph.grid(m)\n","    dist, angle, idx = graph.distance_sklearn_metrics(z, k=FLAGS.number_edges, metric=FLAGS.metric) \n","    #dist contains the distance of the 8 nearest neighbors for each node sorted in ascending order\n","    #idx contains the indexes of the 8 nearest for each node sorted in ascending order by distance\n","\n","    rhos = get_rho(dist, idx)\n","    rhos.setdiag(0.0)\n","    rhos = rhos.tocoo()\n","\n","    theta = get_theta(angle, idx)\n","    theta.setdiag(0.0)\n","    theta = theta.tocoo()\n","    return rhos, theta\n","  \n","\n","def get_laplacian(m):\n","    z = graph.grid(m)\n","    dist, angle, idx = graph.distance_sklearn_metrics(z, k=FLAGS.number_edges, metric=FLAGS.metric) \n","    #dist contains the distance of the 8 nearest neighbors for each node sorted in ascending order\n","    #idx contains the indexes of the 8 nearest for each node sorted in ascending order by distance\n","\n","    A = graph.adjacency(dist, idx)\n","    L = graph.laplacian(A, normalized=FLAGS.normalized_laplacian)\n","    L = L.tocoo()\n","    return L\n","\n","pool_step = 2\n","new_dim = 56\n","np.random.seed(0)\n","rho1, theta1 = grid_graph(new_dim)\n","rho2, theta2 = grid_graph(new_dim//pool_step)\n","rho3, theta3 = grid_graph(new_dim//pool_step**2)\n","Rhos = []\n","Rhos.append(rho1)\n","Rhos.append(rho2)\n","Rhos.append(rho3)\n","Thetas = []\n","Thetas.append(theta1)\n","Thetas.append(theta2)\n","Thetas.append(theta3)\n","\n","L = []\n","L.append(get_laplacian(new_dim))\n","L.append(get_laplacian(new_dim//pool_step))\n","L.append(get_laplacian(new_dim//pool_step**2))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n","  self._set_arrayXarray(i, j, x)\n","/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n","  self._set_arrayXarray(i, j, x)\n","/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n","  self._set_arrayXarray(i, j, x)\n","/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n","  self._set_arrayXarray(i, j, x)\n","/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n","  self._set_arrayXarray(i, j, x)\n","/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n","  self._set_arrayXarray(i, j, x)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"j3jHLHWMuVh-","colab_type":"code","outputId":"8c5ac790-0687-4f46-98c2-e0036eb08c0a","executionInfo":{"status":"ok","timestamp":1581504539891,"user_tz":-60,"elapsed":7735,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":377}},"source":["#loading of MNIST dataset\n","\n","from tensorflow.examples.tutorials.mnist import input_data\n","mnist = input_data.read_data_sets(FLAGS.dir_data, one_hot=False)\n","\n","train_data = mnist.train.images.astype(np.float32)\n","val_data = mnist.validation.images.astype(np.float32) #the first 5K samples of the training dataset \n","                                                      #are used for validation\n","test_data = mnist.test.images.astype(np.float32)\n","train_labels = mnist.train.labels\n","val_labels = mnist.validation.labels\n","test_labels = mnist.test.labels\n","\n","new_train_data = np.zeros((train_data.shape[0], new_dim*new_dim))\n","new_val_data = np.zeros((val_data.shape[0], new_dim*new_dim))\n","new_test_data = np.zeros((test_data.shape[0], new_dim*new_dim))\n","for i in range(train_data.shape[0]):\n","    im = train_data[i,:].reshape(28,28)\n","    im = cv2.resize(im, (new_dim,new_dim), interpolation = cv2.INTER_LINEAR)\n","    new_train_data[i,:] = im.reshape(1,-1)\n","for i in range(val_data.shape[0]):\n","    im = val_data[i,:].reshape(28,28)\n","    im = cv2.resize(im, (new_dim,new_dim), interpolation = cv2.INTER_LINEAR)\n","    new_val_data[i,:] = im.reshape(1,-1)\n","for i in range(test_data.shape[0]):\n","    im = test_data[i,:].reshape(28,28)\n","    im = cv2.resize(im, (new_dim,new_dim), interpolation = cv2.INTER_LINEAR)\n","    new_test_data[i,:] = im.reshape(1,-1)\n","\n","t_start = time.time()\n","train_data = new_train_data\n","val_data = new_val_data\n","test_data = new_test_data\n","print('Execution time: {:.2f}s'.format(time.time() - t_start))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-6-77dd2a6620ec>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please write your own downloading logic.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting data_mnist/train-images-idx3-ubyte.gz\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting data_mnist/train-labels-idx1-ubyte.gz\n","Extracting data_mnist/t10k-images-idx3-ubyte.gz\n","Extracting data_mnist/t10k-labels-idx1-ubyte.gz\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","Execution time: 0.00s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CODfm308uViB","colab_type":"code","colab":{}},"source":["class MoNet:\n","    \"\"\"\n","    The neural network model.\n","    \"\"\"\n","    \n","    #Helper functions used for constructing the model\n","    def _weight_variable(self, shape, regularization=True): \n","        \"\"\"Initializer for the weights\"\"\"\n","        \n","        initial = tf.truncated_normal_initializer(0, 0.1)\n","        var = tf.get_variable('weights', shape, tf.float32, initializer=initial)\n","        if regularization: #append the loss of the current variable to the regularization term \n","            self.regularizers.append(tf.nn.l2_loss(var))\n","        return var\n","    \n","    def _bias_variable(self, shape, regularization=True):\n","        \"\"\"Initializer for the bias\"\"\"\n","        \n","        initial = tf.constant_initializer(0.1)\n","        var = tf.get_variable('bias', shape, tf.float32, initializer=initial)\n","        if regularization:\n","            self.regularizers.append(tf.nn.l2_loss(var))\n","        return var\n","    def _mu_variable(self, shape, regularization=True): \n","        \"\"\"Initializer for the weights\"\"\"\n","        \n","        initial = tf.random_uniform_initializer(0,0.1)\n","        var = tf.get_variable('gaussian_mu', shape, tf.float32, initializer=initial)\n","        if regularization: #append the loss of the current variable to the regularization term \n","            self.regularizers.append(tf.nn.l2_loss(var))\n","        return var  \n","    def _sigma_variable(self, shape, regularization=True): \n","        \"\"\"Initializer for the weights\"\"\"\n","        \n","        initial = tf.random_uniform_initializer(0,1)\n","        var = tf.get_variable('gaussian_sigma', shape, tf.float32, initializer=initial)\n","        if regularization: #append the loss of the current variable to the regularization term \n","            self.regularizers.append(tf.nn.l2_loss(var))\n","        return var \n","\n","    def frobenius_norm(self, tensor): \n","        \"\"\"Computes the frobenius norm for a given tensor\"\"\"\n","        \n","        square_tensor = tf.square(tensor)\n","        tensor_sum = tf.reduce_sum(square_tensor)\n","        frobenius_norm = tf.sqrt(tensor_sum)\n","        return frobenius_norm\n","    \n","    \n","    def count_no_weights(self):\n","        total_parameters = 0\n","        for variable in tf.trainable_variables():\n","            # shape is an array of tf.Dimension\n","            shape = variable.get_shape()\n","            variable_parameters = 1\n","            for dim in shape:\n","                variable_parameters *= dim.value\n","            total_parameters += variable_parameters\n","        print('#weights in the model: %d' % (total_parameters,))\n","\n","    \n","    #Modules used by the graph convolutional network\n","    def MoConv(self, x, L, Rho, Theta, Fout, n_gaussian): \n","        \"\"\"Applies Monet over the graph (i.e. it makes a spatial convolution)\"\"\"\n","        #Fout is the number of output features\n","        #Rho is pseudo-coordinates\n","        #n_gaussian is dimensionality of the extracted patch\n","        \n","        N, M, Fin = x.get_shape()  # N is the number of images\n","                                   # M the number of vertices in the images\n","                                   # Fin the number of features\n","        N, M, Fin = int(N), int(M), int(Fin)\n","        \n","        gaussian_filter = self.gaussianlayer(L, Rho, Theta, n_gaussian) #(M*n_gaussian, M)\n","        W = self._weight_variable([n_gaussian*Fin, Fout], regularization=False) #(n_gaussian*Fin, Fout)\n","        x = tf.transpose(x,[0,2,1]) \n","        x = tf.reshape(x,[N*Fin, M])\n","        x = tf.transpose(x,[1,0]) #(M, N*Fin)\n","        gaussian_feats = tf.matmul(gaussian_filter, x) #(M*n_gaussian, N*Fin)\n","        gaussian_feats = tf.transpose(gaussian_feats, [1,0])\n","        gaussian_feats = tf.reshape(gaussian_feats, [N, Fin,M*n_gaussian])\n","        gaussian_feats = tf.transpose(gaussian_feats, [0,2,1]) \n","        gaussian_feats = tf.reshape(gaussian_feats, [N, M, Fin*n_gaussian])\n","        gaussian_feats = tf.reshape(gaussian_feats, [N*M, Fin*n_gaussian])\n","        output = tf.matmul(gaussian_feats,W) #(N*M, Fout)\n","        output = tf.reshape(output, [N, M, Fout])\n","        return output\n","    \n","    def gaussianlayer(self, L, Rho, Theta, n_gaussian):\n","        #Rho is pseudo-coordinates = rho and theta\n","        #n_gaussian is dimensionality of the extracted patch (should be 2?)\n","        \n","        M, _ = Rho.shape #(each row corresponds to 1 node, containing the neighborhood of the node)\n","        M = int(M)\n","        mask = tf.sparse.SparseTensor(L.indices, [1.0]*(L.values.shape[0]), [M,M])\n","        mask = tf.sparse.to_dense(mask)\n","        mask = tf.expand_dims(mask,2)\n","        with tf.variable_scope('Rho'):\n","            Rho = tf.sparse.to_dense(Rho)\n","            Rho = tf.expand_dims(Rho, 2)\n","            #initialize mu and sigma\n","            rho_mu = tf.constant(0.0, shape=[1, 1, n_gaussian])\n","            rho_sigma = self._sigma_variable([1, 1, n_gaussian], regularization=True)\n","            #compute coefficience of MoNet patch operator\n","            g_weight_rho = tf.exp((Rho - rho_mu)**2*(-0.5/rho_sigma**2))\n","            #mask\n","            g_weight_rho = g_weight_rho * mask \n","        with tf.variable_scope('Theta'):\n","            Theta = tf.sparse.to_dense(Theta)\n","            Theta = tf.expand_dims(Theta, 2)\n","            #initialize mu and sigma\n","            theta_mu = tf.constant(0.0, shape=[1, 1, n_gaussian])\n","            theta_sigma = self._sigma_variable([1, 1, n_gaussian], regularization=True)\n","            #compute coefficience of MoNet patch operator\n","            g_weight_theta = tf.exp((Theta - theta_mu)**2*(-0.5/theta_sigma**2))\n","            #mask\n","            g_weight_theta = g_weight_theta * mask \n","        #normolize\n","        g_weight = g_weight_theta * g_weight_rho\n","        g_weight = g_weight/tf.reduce_sum(g_weight,1,keepdims=True)\n","        #reshape\n","        g_weight = tf.transpose(g_weight,(1,0,2))\n","        g_weight = tf.reshape(g_weight,(M, M*n_gaussian))\n","        return tf.transpose(g_weight,(1,0))\n","    \n","    def b1relu(self, x):\n","        \"\"\"Applies bias and ReLU. One bias per filter.\"\"\"\n","        N, M, F = x.get_shape()\n","        b = self._bias_variable([1, 1, int(F)], regularization=False)\n","        return tf.nn.relu(x + b) #add the bias to the convolutive layer\n","\n","\n","    def mpool1(self, x, p):\n","        \"\"\"Max pooling of size p. Should be a power of 2 (this is possible thanks to the reordering we previously did).\"\"\"\n","        if p > 1:\n","            x = tf.expand_dims(x, 3)  # shape = N x M x F x 1\n","            x = tf.nn.max_pool(x, ksize=[1,p,1,1], strides=[1,p,1,1], padding='SAME')\n","            return tf.squeeze(x, [3])  # shape = N x M/p x F\n","        else:\n","            return x\n","\n","    def fc(self, x, Mout, relu=True):\n","        \"\"\"Fully connected layer with Mout features.\"\"\"\n","        N, Min = x.get_shape()\n","        W = self._weight_variable([int(Min), Mout], regularization=True)\n","        b = self._bias_variable([Mout], regularization=True)\n","        x = tf.matmul(x, W) + b\n","        return tf.nn.relu(x) if relu else x\n","    \n","    #function used for extracting the result of our model\n","    def _inference(self, x, dropout): #definition of the model\n","        \n","        # Graph convolutional layers.\n","        x = tf.expand_dims(x, 2)  # N x M x F=1\n","        rec_size = [new_dim, new_dim//pool_step, new_dim//pool_step**2]\n","        for i in range(len(self.p)):\n","            with tf.variable_scope('cgconv{}'.format(i+1)):\n","                with tf.name_scope('filter'):\n","                    x = self.MoConv(x, self.L[i], self.Rhos[i], self.Thetas[i], self.F[i], self.K[i])\n","                with tf.name_scope('bias_relu'):\n","                    x = self.b1relu(x)\n","                with tf.name_scope('pooling'):\n","                    N, M, F = x.get_shape()\n","                    x = tf.reshape(x, [N, rec_size[i],rec_size[i], F])\n","                    x = tf.contrib.layers.instance_norm(x)\n","                    x = tf.nn.max_pool(x, ksize=[1,self.p[i],self.p[i],1], strides=[1,self.p[i],self.p[i],1], padding='SAME')\n","                    x = tf.reshape(x, [N, -1, F])\n","         \n","        # Fully connected hidden layers.\n","        N, M, F = x.get_shape()\n","        x = tf.reshape(x, [int(N), int(M*F)])  # N x M\n","        if self.M:\n","            for i,M in enumerate(self.M[:-1]): #apply a fully connected layer for each layer defined in M\n","                                              #(we discard the last value in M since it contains the number of classes we have\n","                                              #to predict)\n","                with tf.variable_scope('fc{}'.format(i+1)):\n","                    x = self.fc(x, M)\n","                    x = tf.nn.dropout(x, dropout)\n","            \n","            # Logits linear layer, i.e. softmax without normalization.\n","            with tf.variable_scope('logits'):\n","                x = self.fc(x, self.M[-1], relu=False)\n","        return x\n","    \n","    def convert_coo_to_sparse_tensor(self, L):\n","        indices = np.column_stack((L.row, L.col))\n","        L = tf.SparseTensor(indices, L.data.astype('float32'), L.shape)\n","        L = tf.sparse_reorder(L)\n","        return L\n","    \n","    def __init__(self, p, K, F, M, M_0, batch_size, L, Rhos, Thetas,\n","                 decay_steps, decay_rate, learning_rate=1e-4, momentum=0.9, regularization=5e-4, clip_norm=1e1,\n","                 idx_gpu = '/gpu:0'):\n","        self.regularizers = list() #list of regularization l2 loss for multiple variables\n","        \n","        self.p = p #dimensions of the pooling layers\n","        self.K = K #List of polynomial orders, i.e. filter sizes or number of hops\n","        self.F = F #Number of features of convolutional layers\n","        \n","        self.M = M #Number of neurons in fully connected layers\n","        \n","        self.M_0 = M_0 #number of elements in the first graph \n","        \n","        self.batch_size = batch_size\n","        \n","        #definition of some learning parameters\n","        self.decay_steps = decay_steps\n","        self.decay_rate = decay_rate\n","        self.learning_rate = learning_rate\n","        self.regularization = regularization\n","        \n","        with tf.Graph().as_default() as g:\n","                self.graph = g\n","                tf.set_random_seed(0)\n","                with tf.device(idx_gpu):\n","                        #definition of placeholders\n","                        self.Rhos = [self.convert_coo_to_sparse_tensor(Rho) for Rho in Rhos]\n","                        self.Thetas = [self.convert_coo_to_sparse_tensor(Theta) for Theta in Thetas]\n","                        self.L = [self.convert_coo_to_sparse_tensor(c_L) for c_L in L]\n","                        self.ph_data = tf.placeholder(tf.float32, (self.batch_size, M_0), 'data')\n","                        self.ph_labels = tf.placeholder(tf.int32, (self.batch_size), 'labels')\n","                        self.ph_dropout = tf.placeholder(tf.float32, (), 'dropout')\n","                    \n","                        #Model construction\n","                        self.logits = self._inference(self.ph_data, self.ph_dropout)\n","                        \n","                        #Definition of the loss function\n","                        with tf.name_scope('loss'):\n","                            self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, \n","                                                                                                labels=self.ph_labels)\n","                            self.cross_entropy = tf.reduce_mean(self.cross_entropy)\n","                        with tf.name_scope('regularization'):\n","                            if self.M:\n","                                self.regularization *= tf.add_n(self.regularizers)\n","                        self.loss = self.cross_entropy + self.regularization\n","                        \n","                        # Create a session for running Ops on the Graph.\n","                        config = tf.ConfigProto(allow_soft_placement = True)\n","                        config.gpu_options.allow_growth = True\n","                        self.session = tf.Session(config=config)\n","                        #Solver Definition\n","                        with tf.name_scope('training'):\n","                            # Learning rate.\n","                            self.global_step = tf.Variable(0, name='global_step', trainable=False) #used for counting how many iterations we have done\n","                            if decay_rate != 1: #applies an exponential decay of the lr wrt the number of iterations done\n","                                learning_rate = tf.train.exponential_decay(\n","                                        learning_rate, self.global_step, decay_steps, decay_rate, staircase=True)\n","                            # Optimizer.\n","                            if momentum == 0:\n","                                optimizer = tf.train.AdamOptimizer(learning_rate)\n","                            else: #applies momentum for increasing the robustness of the gradient \n","                                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n","                            #grads = optimizer.compute_gradients(self.loss)\n","                            tvars = tf.trainable_variables()\n","                            #grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), clip_norm)\n","                            variable_names = [v.name for v in tvars]\n","                            variables_to_restore = {v.name.split(\":\")[0]: v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)}\n","                            skip_pretrained_var = []\n","                            variables_to_restore = {v: variables_to_restore[v] for v in variables_to_restore if not any(x in v for x in skip_pretrained_var)}\n","                            if variables_to_restore:\n","                                saver_pre_trained = tf.train.Saver(var_list=variables_to_restore)\n","                                ckpt = tf.train.get_checkpoint_state('./gdrive/My Drive/MNIST_MoNet/model_save/')\n","                                if ckpt != None:\n","                                  saver_pre_trained.restore(self.session, ckpt.model_checkpoint_path) \n","                                  print('Relodaing parameters')\n","                            train_vars = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) if any(x in v.name for x in skip_pretrained_var)]\n","\n","                            grads, variables = zip(*optimizer.compute_gradients(self.loss, var_list=tvars))\n","                            grads, _ = tf.clip_by_global_norm(grads, clip_norm)\n","                            self.op_gradients = optimizer.apply_gradients(zip(grads, variables), \n","                                                                          global_step=self.global_step)\n","                            \n","                        #Computation of the norm gradients (useful for debugging)\n","                        self.var_grad = tf.gradients(self.loss, tf.trainable_variables())\n","                        self.norm_grad = self.frobenius_norm(tf.concat([tf.reshape(g, [-1]) for g in self.var_grad], 0))\n","\n","                        #Extraction of the predictions and computation of accuracy\n","                        self.predictions = tf.cast(tf.argmax(self.logits, dimension=1), tf.int32)\n","                        self.accuracy = 100 * tf.contrib.metrics.accuracy(self.predictions, self.ph_labels)\n","        \n","                        uninitialized_vars = []\n","                        for var in tf.all_variables():\n","                            try:\n","                                self.session.run(var)\n","                            except tf.errors.FailedPreconditionError:\n","                                uninitialized_vars.append(var)\n","\n","                        init_new_vars_op = tf.initialize_variables(uninitialized_vars)\n","                        self.session.run(init_new_vars_op) \n","                        self.saver = tf.train.Saver(tf.global_variables())\n","                        self.count_no_weights()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9n4mLDTFuViE","colab_type":"code","colab":{}},"source":["#Convolutional parameters\n","p = [pool_step, pool_step, new_dim//pool_step**2]   #Dimensions of the pooling layers\n","K = [18, 18, 18] #List of gaussians distribution\n","F = [32, 32, 64] #Number of features of convolutional layers\n","\n","#FC parameters\n","C = max(mnist.train.labels) + 1 #Number of classes we have\n","M = [512, C] #Number of neurons in fully connected layers. If there is no FC, set M = []\n","\n","#Solver parameters\n","batch_size = 100\n","decay_steps = mnist.train.num_examples / batch_size #number of steps to do before decreasing the learning rate\n","decay_rate = 0.95 #how much decreasing the learning rate\n","learning_rate = 0.02\n","momentum = 0.9\n","regularization = 5e-4\n","\n","#Definition of keep probabilities for dropout layers\n","dropout_training = 0.5\n","dropout_val_test = 1.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oE5ekYWHuViG","colab_type":"code","outputId":"6e2c2c4e-9cd6-42cf-a674-cbbd4fdbd02b","executionInfo":{"status":"ok","timestamp":1581504543719,"user_tz":-60,"elapsed":11544,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":445}},"source":["#Construction of the learning obj\n","M_0 = Rhos[0].shape[0] #number of elements in the first graph\n","learning_obj = MoNet(p, K, F, M, M_0, batch_size, L, Rhos, Thetas,\n","                       decay_steps, decay_rate,\n","                       learning_rate=learning_rate, regularization=regularization,\n","                       momentum=momentum)\n","\n","#definition of overall number of training iterations and validation frequency\n","num_iter_val = 550\n","num_total_iter_training = 16501\n","\n","num_iter = 0\n","\n","list_training_loss = list()\n","list_training_norm_grad = list()\n","list_val_accuracy = list()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From <ipython-input-7-62b00d27c674>:178: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From <ipython-input-7-62b00d27c674>:278: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use the `axis` argument instead\n","WARNING:tensorflow:From <ipython-input-7-62b00d27c674>:282: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n","Instructions for updating:\n","Please use tf.global_variables instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py:198: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n","Instructions for updating:\n","Use `tf.variables_initializer` instead.\n","#weights in the model: 94774\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KcKHjZDQuViI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":429},"outputId":"54302ed8-c243-42de-a42a-6b128d0e05d9","executionInfo":{"status":"error","timestamp":1581504761662,"user_tz":-60,"elapsed":229476,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}}},"source":["#training and validation\n","indices = collections.deque() #queue that will contain a permutation of the training indexes\n","for k in range(num_iter, num_total_iter_training):\n","    \n","    #Construction of the training batch\n","    if len(indices) < batch_size: # Be sure to have used all the samples before using one a second time.\n","        indices.extend(np.random.permutation(train_data.shape[0])) #reinitialize the queue of indices\n","    idx = [indices.popleft() for i in range(batch_size)] #extract the current batch of samples\n","    \n","    #data extraction\n","    batch_data, batch_labels = train_data[idx,:], train_labels[idx] \n","    \n","    feed_dict = {learning_obj.ph_data: batch_data, \n","                 learning_obj.ph_labels: batch_labels, \n","                 learning_obj.ph_dropout: dropout_training}\n","    \n","    #Training\n","    tic = time.time()\n","    _, current_training_loss, norm_grad = learning_obj.session.run([learning_obj.op_gradients, \n","                                                                    learning_obj.loss, \n","                                                                    learning_obj.norm_grad], feed_dict = feed_dict) \n","    training_time = time.time() - tic\n","    \n","    list_training_loss.append(current_training_loss)\n","    list_training_norm_grad.append(norm_grad)\n","    \n","    if (np.mod(num_iter, num_iter_val)==0): #validation\n","        msg = \"[TRN] iter = %03i, cost = %3.2e, |grad| = %.2e (%3.2es)\" \\\n","                    % (num_iter, list_training_loss[-1], list_training_norm_grad[-1], training_time)\n","        print(msg)\n","        \n","        #Validation Code\n","        tic = time.time()\n","        val_accuracy = 0\n","        for begin in range(0, val_data.shape[0], batch_size):\n","            end = begin + batch_size\n","            end = min([end, val_data.shape[0]])\n","            \n","            #data extraction\n","            batch_data = np.zeros((end-begin, val_data.shape[1]))\n","            batch_data = val_data[begin:end,:]\n","            batch_labels = np.zeros(batch_size)\n","            batch_labels[:end-begin] = val_labels[begin:end]\n","            \n","            feed_dict = {learning_obj.ph_data: batch_data, \n","                         learning_obj.ph_labels: batch_labels,\n","                         learning_obj.ph_dropout: dropout_val_test}\n","            \n","            batch_accuracy = learning_obj.session.run(learning_obj.accuracy, feed_dict)\n","            val_accuracy += batch_accuracy*batch_data.shape[0]\n","        val_accuracy = val_accuracy/val_data.shape[0]\n","        val_time = time.time() - tic\n","        msg = \"[VAL] iter = %03i, acc = %4.2f (%3.2es)\" % (num_iter, val_accuracy, val_time)\n","        print(msg)\n","        learning_obj.saver.save(learning_obj.session, \"./gdrive/My Drive/MNIST_MoNet/model_save/model.ckpt\", global_step=learning_obj.global_step)\n","    num_iter += 1"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[TRN] iter = 000, cost = 1.20e+01, |grad| = 1.75e+02 (4.69e+00s)\n","[VAL] iter = 000, acc = 13.90 (9.63e+00s)\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-8278d6f809f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     _, current_training_loss, norm_grad = learning_obj.session.run([learning_obj.op_gradients, \n\u001b[1;32m     19\u001b[0m                                                                     \u001b[0mlearning_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                                                                     learning_obj.norm_grad], feed_dict = feed_dict) \n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mtraining_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"HHCVmVk2mPi6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}