{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transfer_CayleyNet.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"YTwCEIkEx285","colab_type":"code","outputId":"6b7e4989-5535-49f9-8594-9740030aec95","executionInfo":{"status":"ok","timestamp":1581508203867,"user_tz":-60,"elapsed":2321,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nqXfChbrx-CD","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append('/content/gdrive/My Drive/MNIST_CayleyNet')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bt4pb_DYpMjD","colab_type":"code","outputId":"1dbf6267-4edd-44c0-8f8e-fcce2cb78c02","executionInfo":{"status":"ok","timestamp":1581508207928,"user_tz":-60,"elapsed":6362,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!pip install scipy==1.0.0"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: scipy==1.0.0 in /usr/local/lib/python3.6/dist-packages (1.0.0)\n","Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.0.0) (1.17.5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EBSa6rBwyCcN","colab_type":"code","outputId":"6713a0fc-7f07-4d11-98ac-7b79d6fa296d","executionInfo":{"status":"ok","timestamp":1581508210095,"user_tz":-60,"elapsed":8515,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":63}},"source":["import tensorflow as tf\n","import time, shutil\n","import numpy as np\n","import os, collections, sklearn\n","import joblib\n","import cv2\n","import graph, coarsening\n","import scipy.sparse as sp\n","\n","import matplotlib.pyplot as plt\n","from scipy.misc import imresize\n","%matplotlib inline"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"27-rdHU5yInB","colab_type":"code","colab":{}},"source":["#Definition of some flags useful later in the code\n","\n","flags = tf.app.flags\n","FLAGS = flags.FLAGS\n","\n","# Graphs.\n","flags.DEFINE_string('f', '', 'kernel')\n","flags.DEFINE_integer('number_edges', 4, 'Graph: minimum number of edges per vertex.')\n","flags.DEFINE_string('metric', 'euclidean', 'Graph: similarity measure (between features).')\n","flags.DEFINE_bool('normalized_laplacian', True, 'Graph Laplacian: normalized.')\n","\n","\n","# Directories.\n","flags.DEFINE_string('dir_data', 'data_mnist', 'Directory to store data.')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1M2EZkMoyOaC","colab_type":"code","outputId":"ff776ffb-7ab6-4017-c417-71d26a8fd052","executionInfo":{"status":"ok","timestamp":1581508210496,"user_tz":-60,"elapsed":8897,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["#Here we proceed at computing the original grid where the images live and the various coarsening that are applied\n","#for each level\n","new_dim = 24\n","pool_step = 2\n","def laplacian2D(N, h):\n","    diag=np.ones([N*N])\n","    mat=sp.spdiags([-diag,2*diag,-diag],[-1,0,1],N,N)\n","    mat = mat.tocsr()\n","    mat[0, N-1] = -1.0\n","    mat[N-1, 0] = -1.0\n","    I=sp.eye(N)\n","    return (sp.kron(I,mat,format='csr')+sp.kron(mat,I,format='csr'))/(h**2)\n","L = []\n","L.append(laplacian2D(new_dim, 2.0*(56/new_dim)))\n","L.append(laplacian2D(new_dim//pool_step, 2.0*pool_step*(56/new_dim)))\n","L.append(laplacian2D(new_dim//pool_step**2, 2.0*pool_step**2*(56/new_dim)))\n","L_r = []\n","L_i = []\n","h = 2.0\n","for c_L in L:\n","    c_L = c_L.toarray()\n","    c_L = (h*c_L - 1j*np.eye(c_L.shape[0])).dot(np.linalg.inv(h*c_L + 1j*np.eye(c_L.shape[0])))\n","    L_r.append(c_L.real)\n","    L_i.append(c_L.imag)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/scipy/sparse/compressed.py:742: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n","  SparseEfficiencyWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"mtkO9i9SyRby","colab_type":"code","colab":{}},"source":["#Normalize Laplacian\n","L_norm = []\n","for k in range(len(L)):\n","    L_norm.append(L[k])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8TJFiKUpyXh_","colab_type":"code","outputId":"ee034367-f70d-4c2b-ed3f-853d210da631","executionInfo":{"status":"ok","timestamp":1581508212168,"user_tz":-60,"elapsed":10551,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":377}},"source":["#loading of MNIST dataset\n","\n","from tensorflow.examples.tutorials.mnist import input_data\n","mnist = input_data.read_data_sets(FLAGS.dir_data, one_hot=False)\n","\n","train_data = mnist.train.images.astype(np.float32)\n","val_data = mnist.validation.images.astype(np.float32) #the first 5K samples of the training dataset \n","                                                      #are used for validation\n","test_data = mnist.test.images.astype(np.float32)\n","train_labels = mnist.train.labels\n","val_labels = mnist.validation.labels\n","test_labels = mnist.test.labels\n","new_train_data = np.zeros((train_data.shape[0], new_dim*new_dim))\n","new_val_data = np.zeros((val_data.shape[0], new_dim*new_dim))\n","new_test_data = np.zeros((test_data.shape[0], new_dim*new_dim))\n","for i in range(train_data.shape[0]):\n","    im = train_data[i,:].reshape(28,28)\n","    im = cv2.resize(im, (new_dim,new_dim), interpolation = cv2.INTER_LINEAR)\n","    new_train_data[i,:] = im.reshape(1,-1)\n","for i in range(val_data.shape[0]):\n","    im = val_data[i,:].reshape(28,28)\n","    im = cv2.resize(im, (new_dim,new_dim), interpolation = cv2.INTER_LINEAR)\n","    new_val_data[i,:] = im.reshape(1,-1)\n","for i in range(test_data.shape[0]):\n","    im = test_data[i,:].reshape(28,28)\n","    im = cv2.resize(im, (new_dim,new_dim), interpolation = cv2.INTER_LINEAR)\n","    new_test_data[i,:] = im.reshape(1,-1)\n","\n","t_start = time.time()\n","train_data = new_train_data\n","val_data = new_val_data\n","test_data = new_test_data\n","print('Execution time: {:.2f}s'.format(time.time() - t_start))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-8-febc93d49cc6>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please write your own downloading logic.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting data_mnist/train-images-idx3-ubyte.gz\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting data_mnist/train-labels-idx1-ubyte.gz\n","Extracting data_mnist/t10k-images-idx3-ubyte.gz\n","Extracting data_mnist/t10k-labels-idx1-ubyte.gz\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","Execution time: 0.00s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zm6EKc2CybUR","colab_type":"code","colab":{}},"source":["class CayleyNet:\n","    \"\"\"\n","    The neural network model.\n","    \"\"\"\n","    \n","    #Helper functions used for constructing the model\n","    def _weight_variable(self, shape, regularization=True, name=\"\"): \n","        \"\"\"Initializer for the weights\"\"\"\n","        \n","        initial = tf.truncated_normal_initializer(0, 0.1)\n","        var = tf.get_variable('weights'+name, shape, tf.float32, initializer=initial)\n","        if regularization: #append the loss of the current variable to the regularization term \n","            self.regularizers.append(tf.nn.l2_loss(var))\n","        return var\n","    \n","    def _bias_variable(self, shape, regularization=True):\n","        \"\"\"Initializer for the bias\"\"\"\n","        \n","        initial = tf.constant_initializer(0.1)\n","        var = tf.get_variable('bias', shape, tf.float32, initializer=initial)\n","        if regularization:\n","            self.regularizers.append(tf.nn.l2_loss(var))\n","        return var\n","    \n","    def _h_variable(self, shape, regularization=False, name=''):\n","        \"\"\"Initializer for the zoom parameter h\"\"\"\n","        \n","        initial = tf.random_uniform_initializer()\n","        var = tf.get_variable('h'+name, shape, tf.float32, initializer=initial)\n","        if regularization:\n","            self.regularizers.append(tf.nn.l2_loss(var))\n","        return var\n","\n","    def frobenius_norm(self, tensor): \n","        \"\"\"Computes the frobenius norm for a given laplacian\"\"\"\n","        \n","        square_tensor = tf.square(tensor)\n","        tensor_sum = tf.reduce_sum(square_tensor)\n","        frobenius_norm = tf.sqrt(tensor_sum)\n","        return frobenius_norm\n","    \n","    def compute_sparse_D_inv_indices(self, M):\n","        \"\"\"Computes the indices required for constructing a sparse version of D^-1.\"\"\"\n","        \n","        idx_main_diag = np.tile(np.expand_dims(np.arange(0, 2*M),1), [1, 2])\n","        idx_diag_ur = np.concatenate([np.expand_dims(np.arange(0, M),1), np.expand_dims(np.arange(0, M)+M,1)], 1)\n","        idx_diag_ll = np.concatenate([np.expand_dims(np.arange(0, M)+M,1), np.expand_dims(np.arange(0, M),1)], 1)\n","        idx = np.concatenate([idx_main_diag, idx_diag_ur, idx_diag_ll], 0)\n","        return idx  \n","    \n","    def compute_sparse_R_indices(self, L_off_diag, M):\n","        \"\"\"Computes the indices required for constructing a sparse version of R.\"\"\"\n","        \n","        idx_L = np.asarray(np.where(L_off_diag)).T\n","        idx_L_sh = idx_L + np.expand_dims(np.asarray([M,M]),0)\n","        idx = np.concatenate([idx_L, idx_L_sh])\n","        return idx\n","    \n","    def compute_sparse_numerator_projection_indices(self, L, M):\n","        \"\"\"Computes the indices required for constructing the numerator projection sparse matrix.\"\"\"\n","        \n","        idx_L = np.asarray(np.where(L)).T\n","        idx_L_sh = idx_L + np.expand_dims(np.asarray([M,M]),0)\n","        idx_diag_ur = np.concatenate([np.expand_dims(np.arange(0, M),1), np.expand_dims(np.arange(0, M)+M,1)], 1)\n","        idx_diag_ll = np.concatenate([np.expand_dims(np.arange(0, M)+M,1), np.expand_dims(np.arange(0, M),1)], 1)\n","        idx = np.concatenate([idx_L, idx_L_sh, idx_diag_ur, idx_diag_ll])\n","        return idx\n","    \n","    def chebyshevConv(self, x, L, Fout, K): \n","        \"\"\"Applies chebyshev polynomials over the graph (i.e. it makes a spectral convolution)\"\"\"\n","        \n","        N, M, Fin = x.get_shape()  # N is the number of images\n","                                  # M the number of vertices in the images\n","                                  # Fin the number of features\n","        N, M, Fin = int(N), int(M), int(Fin)\n","        \n","        x = tf.cast(x, 'complex64')\n","        x = tf.transpose(x, [1,2,0])\n","        x = tf.reshape(x, [M, Fin*N])\n","        def concat(x, x_):\n","            x_ = tf.expand_dims(x_, 0)\n","            return tf.concat([x, x_], 0)\n","        output = tf.expand_dims(x, 0)\n","        for i in range(K):\n","            x = tf.matmul(L, x)\n","            output = concat(output, x)\n","  \n","        x = tf.reshape(output, [K+1, M, Fin, N])\n","        x = tf.transpose(x, [3,1,2,0]) # shape = N x M x Fin x (K + 1)\n","        x = tf.reshape(x, [N*M, Fin*(K+1)])  # shape = N*M x Fin*(K + 1)\n","\n","        real_conv_weights = self._weight_variable([Fin*(K+1), Fout], regularization=False, name='_real')#tf.ones([Fin*(self.n_h*K+1), Fout])#self._weight_variable([Fin*(self.n_h*K+1), Fout], regularization=False, name='_real')\n","        imag_conv_weights = self._weight_variable([Fin*(K+1), Fout], regularization=False, name='_imag')#tf.ones([Fin*(self.n_h*K+1), Fout])#self._weight_variable([Fin*(self.n_h*K+1), Fout], regularization=False, name='_imag')\n","\n","        W_pos_exp = tf.complex(real_conv_weights, -imag_conv_weights)\n","        \n","        x_pos_exp_filt = tf.matmul(x, W_pos_exp)\n","        \n","        x_filt = 2*tf.real(x_pos_exp_filt)\n","        return tf.reshape(x_filt, [N, M, Fout])  # N x M x Fout \n","\n","    def cayleyConv(self, x, L_np, Fout, K): \n","        \"\"\"Applies chebyshev polynomials over the graph.\"\"\"\n","        \n","        M, Fin = x.get_shape()[1:] # M the number of samples in the images, Fin the number of features\n","        M, Fin = int(M), int(Fin)\n","        N = tf.shape(x)[0] # N is the number of images\n","        \n","        # Applies cayley transform by means of Jacobi method.\n","        diag_L_np = np.diag(L_np)  # vector containing the diagonal of L\n","        L_off_diag_np = L_np - np.diag(diag_L_np) # off-diagonal entries of L \n","        \n","        list_x_pos_exp = [tf.cast(tf.expand_dims(x,0), 'complex64')] # 1 x N x M x F\n","        \n","        for iii in range(self.n_h):  # for every zoom parameter we want to use (typically one).\n","            h = self._h_variable([1,1], regularization=False, name='_h%f' % iii)\n","            self.list_h.append(h)\n","            \n","            # Computes matrices required by Jacobi (https://en.wikipedia.org/wiki/Jacobi_method)\n","            \n","            # To make things more efficient we reprent a complex vector of shape M as real vector of shape 2*M\n","            # where the first M values represent real coefficients while the second M the imaginary ones.\n","            # All the matrices here defined are computed according to such notation (it allows to use sparse matrices\n","            # with TF with complex values).\n","            \n","            # ************************** COMPUTES numerator projection **************************\n","            idx = self.compute_sparse_numerator_projection_indices(L_np, M)\n","            \n","            vals_L = tf.squeeze(h*L_np[np.where(L_np)])\n","            vals = tf.concat([vals_L, vals_L, tf.ones([M,]), -tf.ones([M,])], 0)\n","            \n","            cayley_op_neg_sp = tf.SparseTensor(idx, vals, [M*2, M*2])\n","            cayley_op_neg_sp = tf.sparse_reorder(cayley_op_neg_sp)\n","        \n","            # ************************** COMPUTES D **************************\n","            D_real = tf.squeeze(h*diag_L_np)\n","            D = tf.complex(D_real, tf.ones_like(D_real))\n","            D_inv = tf.pow(D, -tf.ones_like(D)) # vector of M elements <- diagonal of D^-1\n","            \n","            idx = self.compute_sparse_D_inv_indices(M)\n","            vals = tf.concat([tf.real(D_inv), tf.real(D_inv), -tf.imag(D_inv), tf.imag(D_inv)], 0)\n","            \n","            D_inv_ext_sp = tf.SparseTensor(idx, vals, [M*2, M*2])\n","            D_inv_ext_sp = tf.sparse_reorder(D_inv_ext_sp)\n","            \n","            # ************************** COMPUTES R **************************\n","            idx = self.compute_sparse_R_indices(L_off_diag_np, M)\n","            \n","            vals_L = tf.squeeze(h*L_off_diag_np[np.where(L_off_diag_np)])\n","            vals = tf.concat([vals_L, vals_L], 0)\n","            \n","            R_sp = tf.SparseTensor(idx, vals, [M*2, M*2])\n","            R_sp = tf.sparse_reorder(R_sp)\n","            \n","            # Applies Jacobi method\n","            c_transform = tf.transpose(x, [1,0,2]) # shape = M, N, F\n","            c_transform = tf.reshape(c_transform, [M, -1]) # shape = M, N*F\n","            last_sol = tf.concat([c_transform, tf.zeros_like(c_transform)],0)\n","            for k in range(K):  # for every order of our polynomial\n","                \n","                # Jacobi initialization\n","                b = tf.sparse_tensor_dense_matmul(cayley_op_neg_sp, last_sol) # shape = M, N*F\n","                a = tf.sparse_tensor_dense_matmul(D_inv_ext_sp, b) # shape = M, N*F\n","                \n","                # Jacobi iterations\n","                cond = lambda i, _: tf.less(i, self.num_jacobi_iter)\n","                body = lambda i, c_sol: [tf.add(i, 1), a  - tf.sparse_tensor_dense_matmul(D_inv_ext_sp, \n","                                                                                          tf.sparse_tensor_dense_matmul(R_sp, c_sol))]\n","                \n","                c_sol = tf.while_loop(cond, body, [0, a], parallel_iterations=1, swap_memory=True)\n","                c_sol = c_sol[-1]\n","                    \n","                # Constructs and saves the final complex matrices\n","                c_sol_complex = tf.complex(c_sol[:M,:], c_sol[M:, :]) #M x N*F\n","                c_sol_reshaped = tf.reshape(c_sol_complex, [M, -1, Fin])\n","                c_sol_reshaped = tf.transpose(c_sol_reshaped, [1, 0, 2]) #N x M x F\n","                list_x_pos_exp.append(tf.expand_dims(c_sol_reshaped,0)) #1 x N x M x Flist_x_pos_exp\n","                \n","                last_sol = c_sol\n","        x_pos_exp = tf.concat(list_x_pos_exp, 0) # shape = n_h*K x N x M x Fin\n","        x_pos_exp = tf.transpose(x_pos_exp, [1,2,0,3])  #N x M x n_h*K x Fin\n","        x_pos_exp = tf.reshape(x_pos_exp, [N*M, -1]) #N*M x 2*K*Fin\n","        \n","        real_conv_weights = self._weight_variable([Fin*(self.n_h*K+1), Fout], regularization=False, name='_real')#tf.ones([Fin*(self.n_h*K+1), Fout])#self._weight_variable([Fin*(self.n_h*K+1), Fout], regularization=False, name='_real')\n","        imag_conv_weights = self._weight_variable([Fin*(self.n_h*K+1), Fout], regularization=False, name='_imag')#tf.ones([Fin*(self.n_h*K+1), Fout])#self._weight_variable([Fin*(self.n_h*K+1), Fout], regularization=False, name='_imag')\n","        \n","        W_pos_exp = tf.complex(real_conv_weights, -imag_conv_weights)\n","        \n","        x_pos_exp_filt = tf.matmul(x_pos_exp, W_pos_exp)\n","        \n","        x_filt = 2*tf.real(x_pos_exp_filt)\n","        return tf.reshape(x_filt, [N, M, Fout])\n","\n","\n","    def b1relu(self, x): #sums a bias and applies relu\n","        \"\"\"Bias and ReLU. One bias per filter.\"\"\"\n","        N, M, F = x.get_shape()\n","        b = self._bias_variable([1, 1, int(F)], regularization=False)\n","        return tf.nn.relu(x + b) #add the bias to the convolutive layer\n","\n","\n","    def mpool1(self, x, p): #efficient pooling realized thanks to the reordering of the laplacians we have done a priori\n","        \"\"\"Max pooling of size p. Should be a power of 2.\"\"\"\n","        if p > 1:\n","            x = tf.expand_dims(x, 3)  # N x M x F x 1\n","            x = tf.nn.max_pool(x, ksize=[1,p,1,1], strides=[1,p,1,1], padding='SAME')\n","            return tf.squeeze(x, [3])  # N x M/p x F\n","        else:\n","            return x\n","    \n","\n","    def b1relu(self, x): #sums a bias and applies relu\n","        \"\"\"Bias and ReLU. One bias per filter.\"\"\"\n","        N, M, F = x.get_shape()\n","        b = self._bias_variable([1, 1, int(F)], regularization=False)\n","        return tf.nn.relu(x + b) #add the bias to the convolutive layer\n","\n","\n","    def mpool1(self, x, p): #efficient pooling realized thanks to the reordering of the laplacians we have done a priori\n","        \"\"\"Max pooling of size p. Should be a power of 2.\"\"\"\n","        if p > 1:\n","            x = tf.expand_dims(x, 3)  # N x M x F x 1\n","            x = tf.nn.max_pool(x, ksize=[1,p,1,1], strides=[1,p,1,1], padding='SAME')\n","            return tf.squeeze(x, [3])  # N x M/p x F\n","        else:\n","            return x\n","\n","    def fc(self, x, Mout, relu=True):\n","        \"\"\"Fully connected layer with Mout features.\"\"\"\n","        N, Min = x.get_shape()\n","        W = self._weight_variable([int(Min), Mout], regularization=True)\n","        b = self._bias_variable([Mout], regularization=True)\n","        x = tf.matmul(x, W) + b\n","        return tf.nn.relu(x) if relu else x\n","      \n","    def count_no_weights(self):\n","        total_parameters = 0\n","        for variable in tf.trainable_variables():\n","            # shape is an array of tf.Dimension\n","            shape = variable.get_shape()\n","            variable_parameters = 1\n","            for dim in shape:\n","                variable_parameters *= dim.value\n","            total_parameters += variable_parameters\n","        print('#weights in the model: %d' % (total_parameters,))\n","    \n","    #function used for extracting the result of our model\n","    def _inference(self, x, dropout): #definition of the model\n","        \n","        # Graph convolutional layers.\n","        x = tf.expand_dims(x, 2)  # N x M x F=1\n","        j = 0\n","        self.list_h = list()\n","        rec_size = [new_dim, new_dim//pool_step, new_dim//(pool_step**2)]\n","        for i in range(len(self.p)):\n","            with tf.variable_scope('cgconv{}'.format(i+1)):\n","                with tf.name_scope('filter'):\n","                    x = self.chebyshevConv(x, self.L_np[i], self.F[i], self.K[i])\n","                    if (i==0):\n","                        self.debug = x\n","                with tf.name_scope('bias_relu'):\n","                    x = self.b1relu(tf.cast(tf.real(x), 'float32'))\n","                with tf.name_scope('pooling'):\n","                    N, M, F = x.get_shape()\n","                    x = tf.reshape(x, [N, rec_size[i],rec_size[i], F])\n","                    x = tf.contrib.layers.instance_norm(x)\n","                    x = tf.nn.max_pool(x, ksize=[1,self.p[i],self.p[i],1], strides=[1,self.p[i],self.p[i],1], padding='SAME')\n","                    x = tf.reshape(x, [N, -1, F])\n","            j += int(np.log2(self.p[i])) if self.p[i] > 1 else 0\n","        \n","        # Fully connected hidden layers.\n","        _, M, F = x.get_shape()\n","        x = tf.reshape(x, [-1, int(M*F)])  # N x M\n","        if self.M:\n","            for i,M in enumerate(self.M[:-1]): #apply a fully connected layer for each layer defined in M\n","                                              #(we discard the last value in M since it contains the number of classes we have\n","                                              #to predict)\n","                with tf.variable_scope('fc{}'.format(i+1)):\n","                    x = self.fc(x, M)\n","                    x = tf.nn.dropout(x, dropout)\n","            \n","            # Logits linear layer, i.e. softmax without normalization.\n","            with tf.variable_scope('logits'):\n","                x = self.fc(x, self.M[-1], relu=False)\n","        return x\n","    \n","    def __init__(self, p, K, F, M, M_0, batch_size, num_jacobi_iter, L_r, L_i,\n","                 decay_steps, decay_rate, learning_rate=1e-4, momentum=0.9, regularization=5e-4, clip_norm=1e1,\n","                 idx_gpu = '/gpu:0'):\n","        self.regularizers = list() #list of regularization l2 loss for multiple variables\n","        self.n_h = 1\n","        self.num_jacobi_iter = num_jacobi_iter\n","        self.p = p #dimensions of the pooling layers\n","        self.K = K #List of polynomial orders, i.e. filter sizes or number of hops\n","        self.F = F #Number of features of convolutional layers\n","        \n","        self.M = M #Number of neurons in fully connected layers\n","        \n","        self.M_0 = M_0 #number of elements in the first graph \n","        \n","        self.batch_size = batch_size\n","        \n","        #definition of some learning parameters\n","        self.decay_steps = decay_steps\n","        self.decay_rate = decay_rate\n","        self.learning_rate = learning_rate\n","        self.regularization = regularization\n","        \n","        with tf.Graph().as_default() as g:\n","                self.graph = g\n","                tf.set_random_seed(0)\n","                with tf.device(idx_gpu):\n","                        #definition of placeholders\n","                        self.L_np = [tf.complex(tf.convert_to_tensor(c_L_r), tf.convert_to_tensor(c_L_i)) for c_L_r, c_L_i in zip(L_r, L_i)]\n","                        self.L_np = [tf.cast(c_L, 'complex64') for c_L in self.L_np]\n","                        self.ph_data = tf.placeholder(tf.float32, (self.batch_size, M_0), 'data')\n","                        self.ph_labels = tf.placeholder(tf.int32, (self.batch_size), 'labels')\n","                        self.ph_dropout = tf.placeholder(tf.float32, (), 'dropout')\n","                    \n","                        #Model construction\n","                        self.logits = self._inference(self.ph_data, self.ph_dropout)\n","                        \n","                        #Definition of the loss function\n","                        with tf.name_scope('loss'):\n","                            self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.ph_labels)\n","                            self.cross_entropy = tf.reduce_mean(self.cross_entropy)\n","                        with tf.name_scope('regularization'):\n","                            if self.M:\n","                                self.regularization *= tf.add_n(self.regularizers)\n","                        self.loss = self.cross_entropy + self.regularization\n","                        # Create a session for running Ops on the Graph.\n","                        config = tf.ConfigProto(allow_soft_placement = True)\n","                        config.gpu_options.allow_growth = True\n","                        self.session = tf.Session(config=config)\n","                        #Solver Definition\n","                        with tf.name_scope('training'):\n","                            # Learning rate.\n","                            global_step = tf.Variable(0, name='global_step', trainable=False) #used for counting how many iterations we have done\n","                            if decay_rate != 1: #applies an exponential decay of the lr wrt the number of iterations done\n","                                learning_rate = tf.train.exponential_decay(\n","                                        learning_rate, global_step, decay_steps, decay_rate, staircase=True)\n","                            # Optimizer.\n","                            if momentum == 0:\n","                                optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","                            else: #applies momentum for increasing the robustness of the gradient \n","                                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n","                            #grads = optimizer.compute_gradients(self.loss)\n","                            tvars = tf.trainable_variables()\n","                            #grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), clip_norm)\n","                            variable_names = [v.name for v in tvars]\n","                            variables_to_restore = {v.name.split(\":\")[0]: v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)}\n","                            skip_pretrained_var = []\n","                            variables_to_restore = {v: variables_to_restore[v] for v in variables_to_restore if not any(x in v for x in skip_pretrained_var)}\n","                            if variables_to_restore:\n","                                saver_pre_trained = tf.train.Saver(var_list=variables_to_restore)\n","                                ckpt = tf.train.get_checkpoint_state('/content/gdrive/My Drive/MNIST_CayleyNet/model_save/')\n","                                saver_pre_trained.restore(self.session, ckpt.model_checkpoint_path) \n","                            print('Relodaing parameters')\n","                            train_vars = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) if any(x in v.name for x in skip_pretrained_var)]\n","\n","                            grads, variables = zip(*optimizer.compute_gradients(self.loss, var_list=tvars))\n","                            grads, _ = tf.clip_by_global_norm(grads, clip_norm)\n","                            self.op_gradients = optimizer.apply_gradients(zip(grads, variables), \n","                                                                          global_step=global_step)\n","                            \n","                        #Computation of the norm gradients (useful for debugging)\n","                        self.var_grad = tf.gradients(self.loss, tf.trainable_variables())\n","                        self.norm_grad = self.frobenius_norm(tf.concat([tf.reshape(g, [-1]) for g in self.var_grad], 0))\n","\n","                        #Extraction of the predictions and computation of accuracy\n","                        self.predictions = tf.cast(tf.argmax(self.logits, dimension=1), tf.int32)\n","                        self.accuracy = 100 * tf.contrib.metrics.accuracy(self.predictions, self.ph_labels)\n","        \n","                        uninitialized_vars = []\n","                        for var in tf.all_variables():\n","                            try:\n","                                self.session.run(var)\n","                            except tf.errors.FailedPreconditionError:\n","                                uninitialized_vars.append(var)\n","\n","                        init_new_vars_op = tf.initialize_variables(uninitialized_vars)\n","                        self.session.run(init_new_vars_op)\n","                        self.count_no_weights()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NWA1PswFyhK5","colab_type":"code","colab":{}},"source":["#Convolutional parameters\n","p = [pool_step, pool_step, new_dim//(pool_step**2)]   #Dimensions of the pooling layers\n","K = [9, 9, 9] #List of gaussians distribution\n","F = [32, 32, 64] #Number of features of convolutional layers\n","\n","#FC parameters\n","C = max(train_labels) + 1 # Number of classes we have\n","M = [512, C] # Number of neurons in fully connected layers\n","\n","#Solver parameters\n","batch_size = 100\n","decay_steps = train_data.shape[0] / batch_size # number of steps to do before decreasing the learning rate\n","decay_rate = 0.95\n","learning_rate = 0.01\n","momentum = 0.9\n","regularization = 5e-4\n","\n","# Definition of keep probabilities for dropout layers\n","dropout_training = 0.5\n","dropout_val_test = 1.0\n","\n","num_jacobi_iter = 10"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xo6P6TeYyj4G","colab_type":"code","outputId":"6212f94b-6e54-490a-e536-6b0d7b9116ab","executionInfo":{"status":"ok","timestamp":1581508215617,"user_tz":-60,"elapsed":13978,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":479}},"source":["# Construction of the learning obj\n","M_0 = L[0].shape[0] # number of elements in the first graph\n","learning_obj = CayleyNet(p, K, F, M, M_0, batch_size, num_jacobi_iter, L_r, L_i,\n","                         decay_steps, decay_rate,\n","                         learning_rate=learning_rate, regularization=regularization,\n","                         momentum=momentum)#, clip_norm=100)\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From <ipython-input-9-4968e039699b>:280: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/MNIST_CayleyNet/model_save/model.ckpt-16501\n","Relodaing parameters\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From <ipython-input-9-4968e039699b>:371: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use the `axis` argument instead\n","WARNING:tensorflow:From <ipython-input-9-4968e039699b>:375: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n","Instructions for updating:\n","Please use tf.global_variables instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py:198: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n","Instructions for updating:\n","Use `tf.variables_initializer` instead.\n","#weights in the model: 100874\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Co3ciKXtymaJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9215aafd-5c3e-4342-c8c1-94db12832131","executionInfo":{"status":"ok","timestamp":1581508218228,"user_tz":-60,"elapsed":16577,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}}},"source":["#Validation Code\n","tic = time.time()\n","val_accuracy = 0\n","for begin in range(0, val_data.shape[0], batch_size):\n","    end = begin + batch_size\n","    end = min([end, val_data.shape[0]])\n","\n","    #data extraction\n","    batch_data = np.zeros((end-begin, val_data.shape[1]))\n","    batch_data = val_data[begin:end,:]\n","    batch_labels = np.zeros(batch_size)\n","    batch_labels[:end-begin] = val_labels[begin:end]\n","\n","    feed_dict = {learning_obj.ph_data: batch_data, \n","                  learning_obj.ph_labels: batch_labels,\n","                  learning_obj.ph_dropout: dropout_val_test}\n","\n","    batch_accuracy = learning_obj.session.run(learning_obj.accuracy, feed_dict)\n","    val_accuracy += batch_accuracy*batch_data.shape[0]\n","val_accuracy = val_accuracy/val_data.shape[0]\n","\n","val_time = time.time() - tic\n","msg = \"[VAL] acc = %4.2f (%3.2es)\" % (val_accuracy, val_time)\n","print(msg)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["[VAL] acc = 33.80 (1.96e+00s)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"90G4Pcifyy_W","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}