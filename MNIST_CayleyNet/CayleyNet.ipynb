{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"CayleyNet.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1581505152140,"user_tz":-60,"elapsed":1638,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"id":"zuxORd4DL36C","outputId":"2faa6c9d-5e29-4923-a95b-6f9651fa2005","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RuOfdUzEMD8R","colab":{}},"source":["import sys\n","sys.path.append('/content/gdrive/My Drive/MNIST_CayleyNet')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2KQ9325-sTxG","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"QgBxLlm_fvSo","colab_type":"code","outputId":"fffba59a-6e52-41cb-c8ba-b7ade2e4a984","executionInfo":{"status":"ok","timestamp":1581505155911,"user_tz":-60,"elapsed":5393,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["!pip install scipy==1.0.0"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: scipy==1.0.0 in /usr/local/lib/python3.6/dist-packages (1.0.0)\n","Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.0.0) (1.17.5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"plZnyBXGL0KO","outputId":"acc853b1-952d-4978-f362-a33f4cadb495","executionInfo":{"status":"ok","timestamp":1581505157884,"user_tz":-60,"elapsed":7355,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":80}},"source":["import tensorflow as tf\n","import time, shutil\n","import numpy as np\n","import os, collections, sklearn\n","import joblib\n","import cv2\n","import graph\n","import coarsening\n","import scipy.sparse as sp\n","\n","import matplotlib.pyplot as plt\n","from scipy.misc import imresize\n","%matplotlib inline"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PZckK28uL0KU"},"source":["# Graph definition and coarsening"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GnckcT9-L0KV","colab":{}},"source":["#Definition of some flags useful later in the code\n","\n","flags = tf.app.flags\n","FLAGS = flags.FLAGS\n","\n","# Graphs.\n","flags.DEFINE_string('f', '', 'kernel')\n","flags.DEFINE_integer('number_edges', 4, 'Graph: minimum number of edges per vertex.')\n","flags.DEFINE_string('metric', 'euclidean', 'Graph: similarity measure (between features).')\n","flags.DEFINE_bool('normalized_laplacian', True, 'Graph Laplacian: normalized.')\n","\n","\n","# Directories.\n","flags.DEFINE_string('dir_data', 'data_mnist', 'Directory to store data.')\n","new_dim = 56\n","pool_step = 2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zVT2tmx7L0Kb","outputId":"26e15328-d782-44a5-cd50-47d4ea7a7d3e","executionInfo":{"status":"ok","timestamp":1581505177524,"user_tz":-60,"elapsed":26981,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["#Here we proceed at computing the original grid where the images live and the various coarsening that are applied\n","#for each level\n","\n","def laplacian2D(N, h):\n","    diag=np.ones([N*N])\n","    mat=sp.spdiags([-diag,2*diag,-diag],[-1,0,1],N,N)\n","    mat = mat.tocsr()\n","    mat[0, N-1] = -1.0\n","    mat[N-1, 0] = -1.0\n","    I=sp.eye(N)\n","    return (sp.kron(I,mat,format='csr')+sp.kron(mat,I,format='csr'))/(h**2)\n","L = []\n","L.append(laplacian2D(56, 2.0))\n","L.append(laplacian2D(new_dim//pool_step, 2.0*pool_step))\n","L.append(laplacian2D(new_dim//(pool_step**2), 2.0*pool_step**2))\n","L_r = []\n","L_i = []\n","h = 2.0\n","for c_L in L:\n","    c_L = c_L.toarray()\n","    c_L = (h*c_L - 1j*np.eye(c_L.shape[0])).dot(np.linalg.inv(h*c_L + 1j*np.eye(c_L.shape[0])))\n","    L_r.append(c_L.real)\n","    L_i.append(c_L.imag)\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/scipy/sparse/compressed.py:742: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n","  SparseEfficiencyWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"B7GxkFsxL0Kh"},"source":["# Data loading"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2g5bz9w5L0Ki","outputId":"90ace01f-9e35-4595-9d7f-c75590db8790","executionInfo":{"status":"ok","timestamp":1581505180654,"user_tz":-60,"elapsed":30100,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":377}},"source":["#loading of MNIST dataset\n","\n","from tensorflow.examples.tutorials.mnist import input_data\n","mnist = input_data.read_data_sets(FLAGS.dir_data, one_hot=False)\n","\n","train_data = mnist.train.images.astype(np.float32)\n","val_data = mnist.validation.images.astype(np.float32) #the first 5K samples of the training dataset \n","                                                      #are used for validation\n","test_data = mnist.test.images.astype(np.float32)\n","train_labels = mnist.train.labels\n","val_labels = mnist.validation.labels\n","test_labels = mnist.test.labels\n","new_train_data = np.zeros((train_data.shape[0], new_dim*new_dim))\n","new_val_data = np.zeros((val_data.shape[0], new_dim*new_dim))\n","new_test_data = np.zeros((test_data.shape[0], new_dim*new_dim))\n","for i in range(train_data.shape[0]):\n","    im = train_data[i,:].reshape(28,28)\n","    im = cv2.resize(im, (new_dim,new_dim), interpolation = cv2.INTER_LINEAR)\n","    new_train_data[i,:] = im.reshape(1,-1)\n","for i in range(val_data.shape[0]):\n","    im = val_data[i,:].reshape(28,28)\n","    im = cv2.resize(im, (new_dim,new_dim), interpolation = cv2.INTER_LINEAR)\n","    new_val_data[i,:] = im.reshape(1,-1)\n","for i in range(test_data.shape[0]):\n","    im = test_data[i,:].reshape(28,28)\n","    im = cv2.resize(im, (new_dim,new_dim), interpolation = cv2.INTER_LINEAR)\n","    new_test_data[i,:] = im.reshape(1,-1)\n","\n","t_start = time.time()\n","train_data = new_train_data\n","val_data = new_val_data\n","test_data = new_test_data\n","print('Execution time: {:.2f}s'.format(time.time() - t_start))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-7-febc93d49cc6>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please write your own downloading logic.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting data_mnist/train-images-idx3-ubyte.gz\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting data_mnist/train-labels-idx1-ubyte.gz\n","Extracting data_mnist/t10k-images-idx3-ubyte.gz\n","Extracting data_mnist/t10k-labels-idx1-ubyte.gz\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","Execution time: 0.00s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lHkUeW0ZL0Kk"},"source":["# Model definition"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Zc1yFBpDL0Kk","colab":{}},"source":["class CayleyNet:\n","    \"\"\"\n","    The neural network model.\n","    \"\"\"\n","    \n","    #Helper functions used for constructing the model\n","    def _weight_variable(self, shape, regularization=True, name=\"\"): \n","        \"\"\"Initializer for the weights\"\"\"\n","        \n","        initial = tf.truncated_normal_initializer(0, 0.1)\n","        var = tf.get_variable('weights'+name, shape, tf.float32, initializer=initial)\n","        if regularization: #append the loss of the current variable to the regularization term \n","            self.regularizers.append(tf.nn.l2_loss(var))\n","        return var\n","    \n","    def _bias_variable(self, shape, regularization=True):\n","        \"\"\"Initializer for the bias\"\"\"\n","        \n","        initial = tf.constant_initializer(0.1)\n","        var = tf.get_variable('bias', shape, tf.float32, initializer=initial)\n","        if regularization:\n","            self.regularizers.append(tf.nn.l2_loss(var))\n","        return var\n","    \n","    def _h_variable(self, shape, regularization=False, name=''):\n","        \"\"\"Initializer for the zoom parameter h\"\"\"\n","        \n","        initial = tf.random_uniform_initializer()\n","        var = tf.get_variable('h'+name, shape, tf.float32, initializer=initial)\n","        if regularization:\n","            self.regularizers.append(tf.nn.l2_loss(var))\n","        return var\n","\n","    def frobenius_norm(self, tensor): \n","        \"\"\"Computes the frobenius norm for a given laplacian\"\"\"\n","        \n","        square_tensor = tf.square(tensor)\n","        tensor_sum = tf.reduce_sum(square_tensor)\n","        frobenius_norm = tf.sqrt(tensor_sum)\n","        return frobenius_norm\n","    \n","    def compute_sparse_D_inv_indices(self, M):\n","        \"\"\"Computes the indices required for constructing a sparse version of D^-1.\"\"\"\n","        \n","        idx_main_diag = np.tile(np.expand_dims(np.arange(0, 2*M),1), [1, 2])\n","        idx_diag_ur = np.concatenate([np.expand_dims(np.arange(0, M),1), np.expand_dims(np.arange(0, M)+M,1)], 1)\n","        idx_diag_ll = np.concatenate([np.expand_dims(np.arange(0, M)+M,1), np.expand_dims(np.arange(0, M),1)], 1)\n","        idx = np.concatenate([idx_main_diag, idx_diag_ur, idx_diag_ll], 0)\n","        return idx  \n","    \n","    def compute_sparse_R_indices(self, L_off_diag, M):\n","        \"\"\"Computes the indices required for constructing a sparse version of R.\"\"\"\n","        \n","        idx_L = np.asarray(np.where(L_off_diag)).T\n","        idx_L_sh = idx_L + np.expand_dims(np.asarray([M,M]),0)\n","        idx = np.concatenate([idx_L, idx_L_sh])\n","        return idx\n","    \n","    def compute_sparse_numerator_projection_indices(self, L, M):\n","        \"\"\"Computes the indices required for constructing the numerator projection sparse matrix.\"\"\"\n","        \n","        idx_L = np.asarray(np.where(L)).T\n","        idx_L_sh = idx_L + np.expand_dims(np.asarray([M,M]),0)\n","        idx_diag_ur = np.concatenate([np.expand_dims(np.arange(0, M),1), np.expand_dims(np.arange(0, M)+M,1)], 1)\n","        idx_diag_ll = np.concatenate([np.expand_dims(np.arange(0, M)+M,1), np.expand_dims(np.arange(0, M),1)], 1)\n","        idx = np.concatenate([idx_L, idx_L_sh, idx_diag_ur, idx_diag_ll])\n","        return idx\n","    def chebyshevConv(self, x, L, Fout, K): \n","        \"\"\"Applies chebyshev polynomials over the graph (i.e. it makes a spectral convolution)\"\"\"\n","        \n","        N, M, Fin = x.get_shape()  # N is the number of images\n","                                  # M the number of vertices in the images\n","                                  # Fin the number of features\n","        N, M, Fin = int(N), int(M), int(Fin)\n","        \n","        x = tf.cast(x, 'complex64')\n","        x = tf.transpose(x, [1,2,0])\n","        x = tf.reshape(x, [M, Fin*N])\n","        def concat(x, x_):\n","            x_ = tf.expand_dims(x_, 0)\n","            return tf.concat([x, x_], 0)\n","        output = tf.expand_dims(x, 0)\n","        for i in range(K):\n","            x = tf.matmul(L, x)\n","            output = concat(output, x)\n","  \n","        x = tf.reshape(output, [K+1, M, Fin, N])\n","        x = tf.transpose(x, [3,1,2,0]) # shape = N x M x Fin x (K + 1)\n","        x = tf.reshape(x, [N*M, Fin*(K+1)])  # shape = N*M x Fin*(K + 1)\n","\n","        real_conv_weights = self._weight_variable([Fin*(K+1), Fout], regularization=False, name='_real')#tf.ones([Fin*(self.n_h*K+1), Fout])#self._weight_variable([Fin*(self.n_h*K+1), Fout], regularization=False, name='_real')\n","        imag_conv_weights = self._weight_variable([Fin*(K+1), Fout], regularization=False, name='_imag')#tf.ones([Fin*(self.n_h*K+1), Fout])#self._weight_variable([Fin*(self.n_h*K+1), Fout], regularization=False, name='_imag')\n","\n","        W_pos_exp = tf.complex(real_conv_weights, -imag_conv_weights)\n","        \n","        x_pos_exp_filt = tf.matmul(x, W_pos_exp)\n","        \n","        x_filt = 2*tf.real(x_pos_exp_filt)\n","        return tf.reshape(x_filt, [N, M, Fout])  # N x M x Fout \n","\n","    def cayleyConv(self, x, L_np, Fout, K): \n","        \"\"\"Applies chebyshev polynomials over the graph.\"\"\"\n","        \n","        M, Fin = x.get_shape()[1:] # M the number of samples in the images, Fin the number of features\n","        M, Fin = int(M), int(Fin)\n","        N = tf.shape(x)[0] # N is the number of images\n","        \n","        # Applies cayley transform by means of Jacobi method.\n","        diag_L_np = np.diag(L_np)  # vector containing the diagonal of L\n","        L_off_diag_np = L_np - np.diag(diag_L_np) # off-diagonal entries of L \n","        \n","        list_x_pos_exp = [tf.cast(tf.expand_dims(x,0), 'complex64')] # 1 x N x M x F\n","        \n","        for iii in range(self.n_h):  # for every zoom parameter we want to use (typically one).\n","            h = self._h_variable([1,1], regularization=False, name='_h%f' % iii)\n","            self.list_h.append(h)\n","            \n","            # Computes matrices required by Jacobi (https://en.wikipedia.org/wiki/Jacobi_method)\n","            \n","            # To make things more efficient we reprent a complex vector of shape M as real vector of shape 2*M\n","            # where the first M values represent real coefficients while the second M the imaginary ones.\n","            # All the matrices here defined are computed according to such notation (it allows to use sparse matrices\n","            # with TF with complex values).\n","            \n","            # ************************** COMPUTES numerator projection **************************\n","            idx = self.compute_sparse_numerator_projection_indices(L_np, M)\n","            \n","            vals_L = tf.squeeze(h*L_np[np.where(L_np)])\n","            vals = tf.concat([vals_L, vals_L, tf.ones([M,]), -tf.ones([M,])], 0)\n","            \n","            cayley_op_neg_sp = tf.SparseTensor(idx, vals, [M*2, M*2])\n","            cayley_op_neg_sp = tf.sparse_reorder(cayley_op_neg_sp)\n","        \n","            # ************************** COMPUTES D **************************\n","            D_real = tf.squeeze(h*diag_L_np)\n","            D = tf.complex(D_real, tf.ones_like(D_real))\n","            D_inv = tf.pow(D, -tf.ones_like(D)) # vector of M elements <- diagonal of D^-1\n","            \n","            idx = self.compute_sparse_D_inv_indices(M)\n","            vals = tf.concat([tf.real(D_inv), tf.real(D_inv), -tf.imag(D_inv), tf.imag(D_inv)], 0)\n","            \n","            D_inv_ext_sp = tf.SparseTensor(idx, vals, [M*2, M*2])\n","            D_inv_ext_sp = tf.sparse_reorder(D_inv_ext_sp)\n","            \n","            # ************************** COMPUTES R **************************\n","            idx = self.compute_sparse_R_indices(L_off_diag_np, M)\n","            \n","            vals_L = tf.squeeze(h*L_off_diag_np[np.where(L_off_diag_np)])\n","            vals = tf.concat([vals_L, vals_L], 0)\n","            \n","            R_sp = tf.SparseTensor(idx, vals, [M*2, M*2])\n","            R_sp = tf.sparse_reorder(R_sp)\n","            \n","            # Applies Jacobi method\n","            c_transform = tf.transpose(x, [1,0,2]) # shape = M, N, F\n","            c_transform = tf.reshape(c_transform, [M, -1]) # shape = M, N*F\n","            last_sol = tf.concat([c_transform, tf.zeros_like(c_transform)],0)\n","            for k in range(K):  # for every order of our polynomial\n","                \n","                # Jacobi initialization\n","                b = tf.sparse_tensor_dense_matmul(cayley_op_neg_sp, last_sol) # shape = M, N*F\n","                a = tf.sparse_tensor_dense_matmul(D_inv_ext_sp, b) # shape = M, N*F\n","                \n","                # Jacobi iterations\n","                cond = lambda i, _: tf.less(i, self.num_jacobi_iter)\n","                body = lambda i, c_sol: [tf.add(i, 1), a  - tf.sparse_tensor_dense_matmul(D_inv_ext_sp, \n","                                                                                          tf.sparse_tensor_dense_matmul(R_sp, c_sol))]\n","                \n","                c_sol = tf.while_loop(cond, body, [0, a], parallel_iterations=1, swap_memory=True)\n","                c_sol = c_sol[-1]\n","                    \n","                # Constructs and saves the final complex matrices\n","                c_sol_complex = tf.complex(c_sol[:M,:], c_sol[M:, :]) #M x N*F\n","                c_sol_reshaped = tf.reshape(c_sol_complex, [M, -1, Fin])\n","                c_sol_reshaped = tf.transpose(c_sol_reshaped, [1, 0, 2]) #N x M x F\n","                list_x_pos_exp.append(tf.expand_dims(c_sol_reshaped,0)) #1 x N x M x Flist_x_pos_exp\n","                \n","                last_sol = c_sol\n","        x_pos_exp = tf.concat(list_x_pos_exp, 0) # shape = n_h*K x N x M x Fin\n","        x_pos_exp = tf.transpose(x_pos_exp, [1,2,0,3])  #N x M x n_h*K x Fin\n","        x_pos_exp = tf.reshape(x_pos_exp, [N*M, -1]) #N*M x 2*K*Fin\n","        \n","        real_conv_weights = self._weight_variable([Fin*(self.n_h*K+1), Fout], regularization=False, name='_real')#tf.ones([Fin*(self.n_h*K+1), Fout])#self._weight_variable([Fin*(self.n_h*K+1), Fout], regularization=False, name='_real')\n","        imag_conv_weights = self._weight_variable([Fin*(self.n_h*K+1), Fout], regularization=False, name='_imag')#tf.ones([Fin*(self.n_h*K+1), Fout])#self._weight_variable([Fin*(self.n_h*K+1), Fout], regularization=False, name='_imag')\n","        \n","        W_pos_exp = tf.complex(real_conv_weights, -imag_conv_weights)\n","        \n","        x_pos_exp_filt = tf.matmul(x_pos_exp, W_pos_exp)\n","        \n","        x_filt = 2*tf.real(x_pos_exp_filt)\n","        return tf.reshape(x_filt, [N, M, Fout])\n","\n","\n","    def b1relu(self, x): #sums a bias and applies relu\n","        \"\"\"Bias and ReLU. One bias per filter.\"\"\"\n","        N, M, F = x.get_shape()\n","        b = self._bias_variable([1, 1, int(F)], regularization=False)\n","        return tf.nn.relu(x + b) #add the bias to the convolutive layer\n","\n","\n","    def mpool1(self, x, p): #efficient pooling realized thanks to the reordering of the laplacians we have done a priori\n","        \"\"\"Max pooling of size p. Should be a power of 2.\"\"\"\n","        if p > 1:\n","            x = tf.expand_dims(x, 3)  # N x M x F x 1\n","            x = tf.nn.max_pool(x, ksize=[1,p,1,1], strides=[1,p,1,1], padding='SAME')\n","            return tf.squeeze(x, [3])  # N x M/p x F\n","        else:\n","            return x\n","    \n","\n","    def b1relu(self, x): #sums a bias and applies relu\n","        \"\"\"Bias and ReLU. One bias per filter.\"\"\"\n","        N, M, F = x.get_shape()\n","        b = self._bias_variable([1, 1, int(F)], regularization=False)\n","        return tf.nn.relu(x + b) #add the bias to the convolutive layer\n","\n","\n","    def mpool1(self, x, p): #efficient pooling realized thanks to the reordering of the laplacians we have done a priori\n","        \"\"\"Max pooling of size p. Should be a power of 2.\"\"\"\n","        if p > 1:\n","            x = tf.expand_dims(x, 3)  # N x M x F x 1\n","            x = tf.nn.max_pool(x, ksize=[1,p,1,1], strides=[1,p,1,1], padding='SAME')\n","            return tf.squeeze(x, [3])  # N x M/p x F\n","        else:\n","            return x\n","\n","    def fc(self, x, Mout, relu=True):\n","        \"\"\"Fully connected layer with Mout features.\"\"\"\n","        N, Min = x.get_shape()\n","        W = self._weight_variable([int(Min), Mout], regularization=True)\n","        b = self._bias_variable([Mout], regularization=True)\n","        x = tf.matmul(x, W) + b\n","        return tf.nn.relu(x) if relu else x\n","    \n","    #function used for extracting the result of our model\n","    def _inference(self, x, dropout): #definition of the model\n","        \n","        # Graph convolutional layers.\n","        x = tf.expand_dims(x, 2)  # N x M x F=1\n","        j = 0\n","        self.list_h = list()\n","        rec_size = [new_dim, new_dim//pool_step, new_dim//(pool_step**2)]\n","        for i in range(len(self.p)):\n","            with tf.variable_scope('cgconv{}'.format(i+1)):\n","                with tf.name_scope('filter'):\n","                    x = self.chebyshevConv(x, self.L_np[i], self.F[i], self.K[i])\n","                    if (i==0):\n","                        self.debug = x\n","                with tf.name_scope('bias_relu'):\n","                    x = self.b1relu(tf.cast(tf.real(x), 'float32'))\n","                with tf.name_scope('pooling'):\n","                    N, M, F = x.get_shape()\n","                    x = tf.reshape(x, [N, rec_size[i],rec_size[i], F])\n","                    x = tf.contrib.layers.instance_norm(x)\n","                    x = tf.nn.max_pool(x, ksize=[1,self.p[i],self.p[i],1], strides=[1,self.p[i],self.p[i],1], padding='SAME')\n","                    x = tf.reshape(x, [N, -1, F])\n","            j += int(np.log2(self.p[i])) if self.p[i] > 1 else 0\n","        \n","        # Fully connected hidden layers.\n","        _, M, F = x.get_shape()\n","        x = tf.reshape(x, [-1, int(M*F)])  # N x M\n","        if self.M:\n","            for i,M in enumerate(self.M[:-1]): #apply a fully connected layer for each layer defined in M\n","                                              #(we discard the last value in M since it contains the number of classes we have\n","                                              #to predict)\n","                with tf.variable_scope('fc{}'.format(i+1)):\n","                    x = self.fc(x, M)\n","                    x = tf.nn.dropout(x, dropout)\n","            \n","            # Logits linear layer, i.e. softmax without normalization.\n","            with tf.variable_scope('logits'):\n","                x = self.fc(x, self.M[-1], relu=False)\n","        return x\n","    \n","    def __init__(self, p, K, F, M, M_0, batch_size, num_jacobi_iter, L_r, L_i,\n","                 decay_steps, decay_rate, learning_rate=1e-4, momentum=0.9, regularization=5e-4, clip_norm=1e1,\n","                 idx_gpu = '/gpu:0'):\n","        self.regularizers = list() #list of regularization l2 loss for multiple variables\n","        self.n_h = 1\n","        self.num_jacobi_iter = num_jacobi_iter\n","        self.p = p #dimensions of the pooling layers\n","        self.K = K #List of polynomial orders, i.e. filter sizes or number of hops\n","        self.F = F #Number of features of convolutional layers\n","        \n","        self.M = M #Number of neurons in fully connected layers\n","        \n","        self.M_0 = M_0 #number of elements in the first graph \n","        \n","        self.batch_size = batch_size\n","        \n","        #definition of some learning parameters\n","        self.decay_steps = decay_steps\n","        self.decay_rate = decay_rate\n","        self.learning_rate = learning_rate\n","        self.regularization = regularization\n","        \n","        with tf.Graph().as_default() as g:\n","                self.graph = g\n","                tf.set_random_seed(0)\n","                with tf.device(idx_gpu):\n","                        #definition of placeholders\n","                        self.L_np = [tf.complex(tf.convert_to_tensor(c_L_r), tf.convert_to_tensor(c_L_i)) for c_L_r, c_L_i in zip(L_r, L_i)]\n","                        self.L_np = [tf.cast(c_L, 'complex64') for c_L in self.L_np]\n","                        self.ph_data = tf.placeholder(tf.float32, (self.batch_size, M_0), 'data')\n","                        self.ph_labels = tf.placeholder(tf.int32, (self.batch_size), 'labels')\n","                        self.ph_dropout = tf.placeholder(tf.float32, (), 'dropout')\n","                    \n","                        #Model construction\n","                        self.logits = self._inference(self.ph_data, self.ph_dropout)\n","                        \n","                        #Definition of the loss function\n","                        with tf.name_scope('loss'):\n","                            self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.ph_labels)\n","                            self.cross_entropy = tf.reduce_mean(self.cross_entropy)\n","                        with tf.name_scope('regularization'):\n","                            if self.M:\n","                              self.regularization *= tf.add_n(self.regularizers)\n","                        self.loss = self.cross_entropy + self.regularization\n","                        # Create a session for running Ops on the Graph.\n","                        config = tf.ConfigProto(allow_soft_placement = True)\n","                        config.gpu_options.allow_growth = True\n","                        self.session = tf.Session(config=config)\n","                        #Solver Definition\n","                        with tf.name_scope('training'):\n","                            # Learning rate.\n","                            self.global_step = tf.Variable(0, name='global_step', trainable=False) #used for counting how many iterations we have done\n","                            if decay_rate != 1: #applies an exponential decay of the lr wrt the number of iterations done\n","                                learning_rate = tf.train.exponential_decay(\n","                                        learning_rate, self.global_step, decay_steps, decay_rate, staircase=True)\n","                            # Optimizer.\n","                            if momentum == 0:\n","                                optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","                            else: #applies momentum for increasing the robustness of the gradient \n","                                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n","                            #grads = optimizer.compute_gradients(self.loss)\n","                            tvars = tf.trainable_variables()\n","                            #grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), clip_norm)\n","                            variable_names = [v.name for v in tvars]\n","                            variables_to_restore = {v.name.split(\":\")[0]: v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)}\n","                            skip_pretrained_var = []\n","                            variables_to_restore = {v: variables_to_restore[v] for v in variables_to_restore if not any(x in v for x in skip_pretrained_var)}\n","                            if variables_to_restore:\n","                                saver_pre_trained = tf.train.Saver(var_list=variables_to_restore)\n","                                ckpt = tf.train.get_checkpoint_state('/content/gdrive/My Drive/MNIST_CayleyNet/model_save/')\n","                                if ckpt != None:\n","                                  saver_pre_trained.restore(self.session, ckpt.model_checkpoint_path) \n","                                  print('Relodaing parameters')\n","                            train_vars = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) if any(x in v.name for x in skip_pretrained_var)]\n","\n","                            grads, variables = zip(*optimizer.compute_gradients(self.loss, var_list=tvars))\n","                            grads, _ = tf.clip_by_global_norm(grads, clip_norm)\n","                            self.op_gradients = optimizer.apply_gradients(zip(grads, variables), \n","                                                                          global_step=self.global_step)\n","                            \n","                        #Computation of the norm gradients (useful for debugging)\n","                        self.var_grad = tf.gradients(self.loss, tf.trainable_variables())\n","                        self.norm_grad = self.frobenius_norm(tf.concat([tf.reshape(g, [-1]) for g in self.var_grad], 0))\n","\n","                        #Extraction of the predictions and computation of accuracy\n","                        self.predictions = tf.cast(tf.argmax(self.logits, dimension=1), tf.int32)\n","                        self.accuracy = 100 * tf.contrib.metrics.accuracy(self.predictions, self.ph_labels)\n","        \n","                        uninitialized_vars = []\n","                        for var in tf.all_variables():\n","                            try:\n","                                self.session.run(var)\n","                            except tf.errors.FailedPreconditionError:\n","                                uninitialized_vars.append(var)\n","\n","                        init_new_vars_op = tf.initialize_variables(uninitialized_vars)\n","                        self.session.run(init_new_vars_op) \n","                        self.saver = tf.train.Saver(tf.global_variables())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"72_JWXtZL0Kl"},"source":["# Training & testing"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"muhM7AJ7L0Km","colab":{}},"source":["#Convolutional parameters\n","p = [pool_step, pool_step, new_dim//(pool_step**2)]   #Dimensions of the pooling layers\n","K = [9, 9, 9] #List of gaussians distribution\n","F = [32, 32, 64] #Number of features of convolutional layers\n","\n","#FC parameters\n","C = max(train_labels) + 1 # Number of classes we have\n","M = [512, C] # Number of neurons in fully connected layers\n","\n","#Solver parameters\n","batch_size = 100\n","decay_steps = train_data.shape[0] / batch_size # number of steps to do before decreasing the learning rate\n","decay_rate = 0.95\n","learning_rate = 0.01\n","momentum = 0.9\n","regularization = 5e-4\n","\n","# Definition of keep probabilities for dropout layers\n","dropout_training = 0.5\n","dropout_val_test = 1.0\n","\n","num_jacobi_iter = 10"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oMic4J0EL0Kn","outputId":"c55fb9f0-523f-489f-fce3-7e051feca8e3","executionInfo":{"status":"ok","timestamp":1581505186606,"user_tz":-60,"elapsed":36032,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":428}},"source":["# Construction of the learning obj\n","M_0 = L[0].shape[0] # number of elements in the first graph\n","learning_obj = CayleyNet(p, K, F, M, M_0, batch_size, num_jacobi_iter, L_r, L_i,\n","                         decay_steps, decay_rate,\n","                         learning_rate=learning_rate, regularization=regularization,\n","                         momentum=momentum)#, clip_norm=100)\n","\n","# definition of overall number of training iterations and validation frequency\n","num_iter_val = 550\n","num_total_iter_training = 16501\n","\n","num_iter = 0\n","\n","list_training_loss = list()\n","list_training_norm_grad = list()\n","list_val_accuracy = list()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From <ipython-input-8-09fc7a603dae>:268: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From <ipython-input-8-09fc7a603dae>:360: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use the `axis` argument instead\n","WARNING:tensorflow:From <ipython-input-8-09fc7a603dae>:364: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n","Instructions for updating:\n","Please use tf.global_variables instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py:198: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n","Instructions for updating:\n","Use `tf.variables_initializer` instead.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"a9d6_1WYL0Ko","scrolled":true,"outputId":"1cdbd849-ac83-4679-bcf9-5421a4e138f9","executionInfo":{"status":"ok","timestamp":1581507456600,"user_tz":-60,"elapsed":444573,"user":{"displayName":"Wei Huang","photoUrl":"","userId":"00807831990072670374"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#training and validation\n","indices = collections.deque() # queue containing a permutation of the training indexes\n","for k in range(num_iter, num_total_iter_training):\n","\n","    #Construction of the training batch\n","    if len(indices) < batch_size: # Be sure to have used all the samples before using one a second time.\n","        indices.extend(np.random.permutation(train_data.shape[0])) #reinitialize the queue of indices\n","    idx = [indices.popleft() for i in range(batch_size)] #extract the current batch of samples\n","\n","    #data extraction\n","    batch_data, batch_labels = train_data[idx,:], train_labels[idx] \n","\n","    feed_dict = {learning_obj.ph_data: batch_data, \n","                 learning_obj.ph_labels: batch_labels, \n","                 learning_obj.ph_dropout: dropout_training}\n","\n","    #Training\n","    tic = time.time()\n","    _, current_training_loss, norm_grad = learning_obj.session.run([learning_obj.op_gradients, \n","                                                                    learning_obj.loss, \n","                                                                    learning_obj.norm_grad], feed_dict = feed_dict) \n","    training_time = time.time() - tic\n","\n","    list_training_loss.append(current_training_loss)\n","    list_training_norm_grad.append(norm_grad)\n","    if (np.mod(num_iter, num_iter_val)==0): #validation\n","        msg = \"[TRN] iter = %03i, cost = %3.2e, |grad| = %.2e (%3.2es)\" \\\n","                    % (num_iter, list_training_loss[-1], list_training_norm_grad[-1], training_time)\n","        print(msg)\n","\n","        #Validation Code\n","        tic = time.time()\n","        val_accuracy = 0\n","        for begin in range(0, val_data.shape[0], batch_size):\n","            end = begin + batch_size\n","            end = min([end, val_data.shape[0]])\n","\n","            #data extraction\n","            batch_data = np.zeros((end-begin, val_data.shape[1]))\n","            batch_data = val_data[begin:end,:]\n","            batch_labels = np.zeros(batch_size)\n","            batch_labels[:end-begin] = val_labels[begin:end]\n","\n","            feed_dict = {learning_obj.ph_data: batch_data, \n","                         learning_obj.ph_labels: batch_labels,\n","                         learning_obj.ph_dropout: dropout_val_test}\n","\n","            batch_accuracy = learning_obj.session.run(learning_obj.accuracy, feed_dict)\n","            val_accuracy += batch_accuracy*batch_data.shape[0]\n","        val_accuracy = val_accuracy/val_data.shape[0]\n","\n","        val_time = time.time() - tic\n","        msg = \"[VAL] iter = %03i, acc = %4.2f (%3.2es)\" % (num_iter, val_accuracy, val_time)\n","        print(msg)\n","        learning_obj.saver.save(learning_obj.session, \"./gdrive/My Drive/MNIST_CayleyNet/model_save/model.ckpt\", global_step=learning_obj.global_step)\n","    num_iter += 1"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[TRN] iter = 000, cost = 9.42e+00, |grad| = 5.20e+01 (6.07e+00s)\n","[VAL] iter = 000, acc = 9.74 (6.45e+00s)\n","[TRN] iter = 550, cost = 7.00e-01, |grad| = 3.60e+00 (1.30e-01s)\n","[VAL] iter = 550, acc = 78.20 (2.33e+00s)\n","[TRN] iter = 1100, cost = 6.70e-01, |grad| = 3.88e+00 (1.30e-01s)\n","[VAL] iter = 1100, acc = 81.64 (2.33e+00s)\n","[TRN] iter = 1650, cost = 4.44e-01, |grad| = 1.95e+00 (1.30e-01s)\n","[VAL] iter = 1650, acc = 85.92 (2.34e+00s)\n","[TRN] iter = 2200, cost = 4.85e-01, |grad| = 2.27e+00 (1.30e-01s)\n","[VAL] iter = 2200, acc = 87.98 (2.33e+00s)\n","[TRN] iter = 2750, cost = 5.03e-01, |grad| = 2.32e+00 (1.29e-01s)\n","[VAL] iter = 2750, acc = 87.46 (2.33e+00s)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","[TRN] iter = 3300, cost = 4.48e-01, |grad| = 2.18e+00 (1.30e-01s)\n","[VAL] iter = 3300, acc = 90.60 (2.34e+00s)\n","[TRN] iter = 3850, cost = 3.03e-01, |grad| = 1.59e+00 (1.30e-01s)\n","[VAL] iter = 3850, acc = 90.74 (2.32e+00s)\n","[TRN] iter = 4400, cost = 3.92e-01, |grad| = 2.09e+00 (1.29e-01s)\n","[VAL] iter = 4400, acc = 90.68 (2.31e+00s)\n","[TRN] iter = 4950, cost = 4.21e-01, |grad| = 2.38e+00 (1.30e-01s)\n","[VAL] iter = 4950, acc = 91.10 (2.33e+00s)\n","[TRN] iter = 5500, cost = 4.00e-01, |grad| = 2.48e+00 (1.30e-01s)\n","[VAL] iter = 5500, acc = 91.10 (2.32e+00s)\n","[TRN] iter = 6050, cost = 4.88e-01, |grad| = 2.59e+00 (1.30e-01s)\n","[VAL] iter = 6050, acc = 91.36 (2.31e+00s)\n","[TRN] iter = 6600, cost = 3.66e-01, |grad| = 2.70e+00 (1.30e-01s)\n","[VAL] iter = 6600, acc = 92.04 (2.32e+00s)\n","[TRN] iter = 7150, cost = 3.32e-01, |grad| = 2.12e+00 (1.31e-01s)\n","[VAL] iter = 7150, acc = 92.26 (2.33e+00s)\n","[TRN] iter = 7700, cost = 4.05e-01, |grad| = 2.73e+00 (1.30e-01s)\n","[VAL] iter = 7700, acc = 91.80 (2.32e+00s)\n","[TRN] iter = 8250, cost = 3.62e-01, |grad| = 2.44e+00 (1.30e-01s)\n","[VAL] iter = 8250, acc = 92.06 (2.33e+00s)\n","[TRN] iter = 8800, cost = 3.91e-01, |grad| = 1.98e+00 (1.30e-01s)\n","[VAL] iter = 8800, acc = 92.28 (2.32e+00s)\n","[TRN] iter = 9350, cost = 2.67e-01, |grad| = 2.14e+00 (1.30e-01s)\n","[VAL] iter = 9350, acc = 92.26 (2.31e+00s)\n","[TRN] iter = 9900, cost = 4.62e-01, |grad| = 2.99e+00 (1.30e-01s)\n","[VAL] iter = 9900, acc = 92.56 (2.31e+00s)\n","[TRN] iter = 10450, cost = 4.32e-01, |grad| = 3.17e+00 (1.30e-01s)\n","[VAL] iter = 10450, acc = 92.44 (2.32e+00s)\n","[TRN] iter = 11000, cost = 3.15e-01, |grad| = 2.48e+00 (1.30e-01s)\n","[VAL] iter = 11000, acc = 92.40 (2.32e+00s)\n","[TRN] iter = 11550, cost = 2.90e-01, |grad| = 2.14e+00 (1.30e-01s)\n","[VAL] iter = 11550, acc = 93.20 (2.31e+00s)\n","[TRN] iter = 12100, cost = 2.96e-01, |grad| = 2.37e+00 (1.30e-01s)\n","[VAL] iter = 12100, acc = 93.16 (2.32e+00s)\n","[TRN] iter = 12650, cost = 3.50e-01, |grad| = 2.40e+00 (1.30e-01s)\n","[VAL] iter = 12650, acc = 92.88 (2.31e+00s)\n","[TRN] iter = 13200, cost = 2.56e-01, |grad| = 2.54e+00 (1.29e-01s)\n","[VAL] iter = 13200, acc = 93.50 (2.31e+00s)\n","[TRN] iter = 13750, cost = 2.26e-01, |grad| = 2.05e+00 (1.30e-01s)\n","[VAL] iter = 13750, acc = 93.16 (2.32e+00s)\n","[TRN] iter = 14300, cost = 1.94e-01, |grad| = 1.45e+00 (1.30e-01s)\n","[VAL] iter = 14300, acc = 93.24 (2.31e+00s)\n","[TRN] iter = 14850, cost = 4.26e-01, |grad| = 2.74e+00 (1.29e-01s)\n","[VAL] iter = 14850, acc = 93.12 (2.31e+00s)\n","[TRN] iter = 15400, cost = 2.75e-01, |grad| = 2.17e+00 (1.30e-01s)\n","[VAL] iter = 15400, acc = 93.62 (2.32e+00s)\n","[TRN] iter = 15950, cost = 4.43e-01, |grad| = 2.94e+00 (1.29e-01s)\n","[VAL] iter = 15950, acc = 93.30 (2.33e+00s)\n","[TRN] iter = 16500, cost = 1.36e-01, |grad| = 1.13e+00 (1.30e-01s)\n","[VAL] iter = 16500, acc = 93.58 (2.32e+00s)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j5WdvO9OH7as","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}